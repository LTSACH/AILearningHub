{
  "category_distribution": {
    "title": "Category Distribution",
    "explanation": {
      "what": "Category distribution shows how articles are spread across different news categories. This visualization helps us understand dataset balance and potential bias.",
      "why": "Understanding category distribution is crucial for detecting class imbalance, deciding on sampling techniques, and choosing appropriate evaluation metrics.",
      "how": "Look for balanced distribution (all categories similar counts) or imbalanced classes (some much larger). Imbalance may require techniques like oversampling, undersampling, or class weights."
    },
    "plotly": "# Load data from URL or upload\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Option 1: Load from URL\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Option 2: Upload in Google Colab\n# from google.colab import files\n# uploaded = files.upload()\n# df = pd.read_csv('bbc-news.csv')\n\n# Count articles per category\ncategory_counts = df['label'].value_counts().sort_index()\n\n# Create pie chart\nfig = go.Figure(data=[go.Pie(\n    labels=category_counts.index,\n    values=category_counts.values,\n    hole=0.3,  # Makes it a donut chart\n    marker=dict(colors=['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b']),\n    textinfo='label+percent',\n    textfont_size=14\n)])\n\nfig.update_layout(\n    title='Category Distribution',\n    showlegend=True,\n    width=600,\n    height=400\n)\n\nfig.show()\n\n# Print statistics\nprint(\"Category Distribution:\")\nprint(category_counts)\nprint(f\"\\nTotal articles: {len(df)}\")\nprint(f\"Most common: {category_counts.idxmax()} ({category_counts.max()} articles)\")\nprint(f\"Least common: {category_counts.idxmin()} ({category_counts.min()} articles)\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data (same options as Plotly)\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Count articles per category\ncategory_counts = df['label'].value_counts().sort_index()\n\n# Create pie chart\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b']\n\nwedges, texts, autotexts = ax.pie(\n    category_counts.values, \n    labels=category_counts.index,\n    autopct='%1.1f%%',\n    colors=colors,\n    startangle=90,\n    textprops={'fontsize': 12}\n)\n\n# Make percentage text bold\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\n\nax.set_title('Category Distribution', fontsize=16, pad=20)\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(\"Category Distribution:\")\nprint(category_counts)\nprint(f\"\\nTotal articles: {len(df)}\")",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Count articles per category\ncategory_counts = df['label'].value_counts().sort_index()\n\n# Create bar plot with seaborn\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b']\n\nsns.barplot(x=category_counts.index, \n           y=category_counts.values,\n           palette=colors,\n           ax=ax)\n\nax.set_title('Category Distribution', fontsize=16, pad=20)\nax.set_xlabel('Category', fontsize=12)\nax.set_ylabel('Number of Articles', fontsize=12)\n\n# Add value labels on bars\nfor i, v in enumerate(category_counts.values):\n    ax.text(i, v + 10, str(v), ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(\"Category Distribution:\")\nfor cat, count in category_counts.items():\n    pct = (count / len(df)) * 100\n    print(f\"{cat:15s}: {count:4d} articles ({pct:5.1f}%)\")"
  },
  "stop_words": {
    "title": "Stop Words Analysis",
    "explanation": {
      "what": "Stop words are common words (the, and, for, etc.) that appear frequently but carry little meaning. This analysis shows which stop words dominate the dataset.",
      "why": "Understanding stop word distribution helps decide preprocessing strategy. Removing stop words reduces dimensionality and focuses on meaningful content.",
      "how": "High stop word percentage (30-50%) is normal for natural text. If much higher, check data quality. If lower, text might be pre-processed already."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Count stop words\nall_words = ' '.join(df['text']).lower().split()\nstop_word_counts = Counter([word for word in all_words if word in stop_words])\ntop_stop_words = stop_word_counts.most_common(20)\n\nwords = [word for word, count in top_stop_words]\ncounts = [count for word, count in top_stop_words]\n\n# Create bar chart\nfig = go.Figure(data=[go.Bar(\n    x=words,\n    y=counts,\n    marker_color='#764ba2',\n    text=counts,\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title='Top 20 Stop Words',\n    xaxis_title='Stop Words',\n    yaxis_title='Frequency',\n    width=900,\n    height=400,\n    showlegend=False\n)\n\nfig.show()\n\n# Statistics\ntotal_words = len(all_words)\ntotal_stop_words = sum(stop_word_counts.values())\nprint(f\"Total words: {total_words:,}\")\nprint(f\"Stop words: {total_stop_words:,} ({total_stop_words/total_words*100:.1f}%)\")\nprint(f\"Unique stop words found: {len(stop_word_counts)}\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Count stop words\nall_words = ' '.join(df['text']).lower().split()\nstop_word_counts = Counter([word for word in all_words if word in stop_words])\ntop_stop_words = stop_word_counts.most_common(20)\n\nwords = [word for word, count in top_stop_words]\ncounts = [count for word, count in top_stop_words]\n\n# Create bar chart\nfig, ax = plt.subplots(figsize=(12, 6))\nax.bar(words, counts, color='#764ba2', alpha=0.7)\n\nax.set_title('Top 20 Stop Words', fontsize=16, pad=20)\nax.set_xlabel('Stop Words', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n# Statistics\ntotal_words = len(all_words)\ntotal_stop_words = sum(stop_word_counts.values())\nprint(f\"Stop words: {total_stop_words:,} ({total_stop_words/total_words*100:.1f}%)\")",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Count stop words\nall_words = ' '.join(df['text']).lower().split()\nstop_word_counts = Counter([word for word in all_words if word in stop_words])\ntop_stop_words = stop_word_counts.most_common(20)\n\n# Create DataFrame for seaborn\nstop_df = pd.DataFrame(top_stop_words, columns=['word', 'count'])\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.barplot(data=stop_df, x='word', y='count', color='#764ba2', ax=ax)\n\nax.set_title('Top 20 Stop Words', fontsize=16, pad=20)\nax.set_xlabel('Stop Words', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
  },
  "word_count_distribution": {
    "title": "Word Count Distribution",
    "explanation": {
      "what": "Distribution of document lengths measured in words. Shows how article lengths vary across the dataset.",
      "why": "Important for setting max_length in models (BERT, etc.), identifying outliers, and understanding memory requirements.",
      "how": "Look for the peak (most common length), range (min/max), and outliers. Use percentiles to set reasonable max_length."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate word counts\ndf['word_count'] = df['text'].str.split().str.len()\n\n# Create histogram bins\nbins = np.arange(0, df['word_count'].max() + 100, 100)\nhist, bin_edges = np.histogram(df['word_count'], bins=bins)\n\n# Format bin labels\nbin_labels = [f\"{int(bin_edges[i])}-{int(bin_edges[i+1])}\" for i in range(len(hist))]\n\n# Create histogram\nfig = go.Figure(data=[go.Bar(\n    x=bin_labels,\n    y=hist,\n    marker_color='#667eea',\n    text=hist,\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title='Word Count Distribution',\n    xaxis_title='Word Count Ranges',\n    yaxis_title='Number of Articles',\n    width=900,\n    height=400,\n    showlegend=False,\n    xaxis={'tickangle': 45}\n)\n\nfig.show()\n\n# Print statistics\nprint(f\"Mean: {df['word_count'].mean():.2f}\")\nprint(f\"Median: {df['word_count'].median():.2f}\")\nprint(f\"Min: {df['word_count'].min()}\")\nprint(f\"Max: {df['word_count'].max()}\")\nprint(f\"\\nPercentiles:\")\nfor p in [25, 50, 75, 90, 95, 99]:\n    print(f\"  {p}th: {df['word_count'].quantile(p/100):.0f}\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate word counts\ndf['word_count'] = df['text'].str.split().str.len()\n\n# Create histogram\nfig, ax = plt.subplots(figsize=(12, 6))\n\nn, bins, patches = ax.hist(df['word_count'], bins=45, color='#667eea', \n                            alpha=0.7, edgecolor='black')\n\nax.set_title('Word Count Distribution', fontsize=16, pad=20)\nax.set_xlabel('Word Count', fontsize=12)\nax.set_ylabel('Number of Articles', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\n# Add vertical line for mean and median\nax.axvline(df['word_count'].mean(), color='red', linestyle='--', \n          linewidth=2, label=f'Mean: {df[\"word_count\"].mean():.0f}')\nax.axvline(df['word_count'].median(), color='green', linestyle='--', \n          linewidth=2, label=f'Median: {df[\"word_count\"].median():.0f}')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(f\"Mean: {df['word_count'].mean():.2f}\")\nprint(f\"Median: {df['word_count'].median():.2f}\")\nprint(f\"Std: {df['word_count'].std():.2f}\")",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate word counts\ndf['word_count'] = df['text'].str.split().str.len()\n\n# Create histogram with KDE\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsns.histplot(data=df, x='word_count', bins=45, color='#667eea', \n            kde=True, ax=ax, alpha=0.7)\n\nax.set_title('Word Count Distribution', fontsize=16, pad=20)\nax.set_xlabel('Word Count', fontsize=12)\nax.set_ylabel('Number of Articles', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\n# Add vertical lines\nax.axvline(df['word_count'].mean(), color='red', linestyle='--', \n          linewidth=2, label=f'Mean: {df[\"word_count\"].mean():.0f}')\nax.axvline(df['word_count'].median(), color='green', linestyle='--', \n          linewidth=2, label=f'Median: {df[\"word_count\"].median():.0f}')\nax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  "char_count_distribution": {
    "title": "Character Count Distribution",
    "explanation": {
      "what": "Distribution of document lengths measured in characters (including spaces).",
      "why": "Useful for storage estimation, API limits (OpenAI has character limits), and processing time prediction.",
      "how": "Compare with word count to estimate average word length. Use for capacity planning."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate character counts\ndf['char_count'] = df['text'].str.len()\n\n# Create histogram bins\nbins = np.arange(0, df['char_count'].max() + 500, 500)\nhist, bin_edges = np.histogram(df['char_count'], bins=bins)\n\nbin_labels = [f\"{int(bin_edges[i])}-{int(bin_edges[i+1])}\" for i in range(len(hist))]\n\n# Create histogram\nfig = go.Figure(data=[go.Bar(\n    x=bin_labels,\n    y=hist,\n    marker_color='#f093fb',\n    text=hist,\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title='Character Count Distribution',\n    xaxis_title='Character Count Ranges',\n    yaxis_title='Number of Articles',\n    width=900,\n    height=400,\n    showlegend=False,\n    xaxis={'tickangle': 45}\n)\n\nfig.show()\n\n# Print statistics\nprint(f\"Mean: {df['char_count'].mean():.2f}\")\nprint(f\"Median: {df['char_count'].median():.2f}\")\nprint(f\"Min: {df['char_count'].min()}\")\nprint(f\"Max: {df['char_count'].max()}\")\n\n# Estimate average word length\navg_word_length = df['char_count'].sum() / df['text'].str.split().str.len().sum()\nprint(f\"\\nAverage word length: {avg_word_length:.2f} characters\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate character counts\ndf['char_count'] = df['text'].str.len()\n\n# Create histogram\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.hist(df['char_count'], bins=50, color='#f093fb', alpha=0.7, edgecolor='black')\n\nax.set_title('Character Count Distribution', fontsize=16, pad=20)\nax.set_xlabel('Character Count', fontsize=12)\nax.set_ylabel('Number of Articles', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(f\"Mean: {df['char_count'].mean():.2f}\")\nprint(f\"Median: {df['char_count'].median():.2f}\")\nprint(f\"Max: {df['char_count'].max()}\")",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate character counts\ndf['char_count'] = df['text'].str.len()\n\n# Create histogram with KDE\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsns.histplot(data=df, x='char_count', bins=50, color='#f093fb', \n            kde=True, ax=ax, alpha=0.7)\n\nax.set_title('Character Count Distribution', fontsize=16, pad=20)\nax.set_xlabel('Character Count', fontsize=12)\nax.set_ylabel('Number of Articles', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  "vocabulary_richness": {
    "title": "Vocabulary Richness by Category",
    "explanation": {
      "what": "Measures vocabulary diversity: total words vs unique words per category. Type-Token Ratio (TTR) = unique/total.",
      "why": "Indicates content complexity and writing style. Higher TTR = more diverse vocabulary. Useful for comparing categories.",
      "how": "Compare unique word counts across categories. Higher values suggest more specialized or diverse content."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate vocabulary richness per category\nvocab_stats = []\nfor category in sorted(df['label'].unique()):\n    cat_df = df[df['label'] == category]\n    \n    # Get all words, remove stop words\n    import re\n    all_text = ' '.join(cat_df['text']).lower()\n    # Use regex to match words >= 3 characters (same as report)\n    words = re.findall(r'\b[a-z]{3,}\b', all_text)\n    words = [w for w in words if w not in stop_words]\n    \n    unique_words = len(set(words))\n    total_words = len(words)\n    ttr = unique_words / total_words if total_words > 0 else 0\n    \n    vocab_stats.append({\n        'category': category,\n        'unique_words': unique_words,\n        'total_words': total_words,\n        'ttr': ttr,\n        'articles': len(cat_df)\n    })\n\nvocab_df = pd.DataFrame(vocab_stats)\n\n# Create bar chart\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x=vocab_df['category'],\n    y=vocab_df['unique_words'],\n    name='Unique Words',\n    marker_color='#4facfe'\n))\n\nfig.update_layout(\n    title='Vocabulary Richness by Category',\n    xaxis_title='Category',\n    yaxis_title='Unique Words Count',\n    width=800,\n    height=400\n)\n\nfig.show()\n\n# Print statistics\nprint(\"\\nVocabulary Richness Statistics:\")\nprint(vocab_df.to_string(index=False))",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate vocabulary richness\nimport re\nvocab_stats = {}\nfor category in sorted(df['label'].unique()):\n    cat_df = df[df['label'] == category]\n    all_text = ' '.join(cat_df['text']).lower()\n    # Use regex to match words >= 3 characters (same as report)\n    words = re.findall(r'\b[a-z]{3,}\b', all_text)\n    words = [w for w in words if w not in stop_words]\n    vocab_stats[category] = len(set(words))\n\n# Create bar chart\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncategories = list(vocab_stats.keys())\nvalues = list(vocab_stats.values())\n\nax.bar(categories, values, color='#4facfe', alpha=0.7)\n\nax.set_title('Vocabulary Richness by Category', fontsize=16, pad=20)\nax.set_xlabel('Category', fontsize=12)\nax.set_ylabel('Unique Words Count', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\n# Add values on bars\nfor i, v in enumerate(values):\n    ax.text(i, v + 100, f'{v:,}', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Calculate vocabulary richness\nvocab_data = []\nfor category in sorted(df['label'].unique()):\n    cat_df = df[df['label'] == category]\n    all_text = ' '.join(cat_df['text']).lower()\n    words = [w for w in all_text.split() if w.isalpha() and w not in stop_words]\n    vocab_data.append({\n        'Category': category,\n        'Unique Words': len(set(words))\n    })\n\nvocab_df = pd.DataFrame(vocab_data)\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(data=vocab_df, x='Category', y='Unique Words', color='#4facfe', ax=ax)\n\nax.set_title('Vocabulary Richness by Category', fontsize=16, pad=20)\nax.set_xlabel('Category', fontsize=12)\nax.set_ylabel('Unique Words Count', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  "category_keywords": {
    "title": "Top 50 Words by Category",
    "explanation": {
      "what": "Shows the most frequent words in each category after removing stop words. These are the words that appear most often in each news topic.",
      "why": "Helps understand what each category talks about and identify key terms that distinguish categories from each other.",
      "how": "Compare word frequencies across categories. High-frequency words reveal main themes. If same words appear in multiple categories, they may not be good discriminators."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Download stop words\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Select a category to analyze\ncategory = 'business'  # Change to: business, entertainment, politics, sport, tech\ncat_df = df[df['label'] == category]\n\n# Get word frequencies (remove stop words)\nimport re\n\nall_text = ' '.join(cat_df['text']).lower()\n# Use regex to match words >= 3 characters (same as report)\nwords = re.findall(r'\b[a-z]{3,}\b', all_text)\nwords = [w for w in words if w not in stop_words]\nword_counts = Counter(words).most_common(20)\n\nwords_list = [word for word, count in word_counts]\ncounts_list = [count for word, count in word_counts]\n\n# Create horizontal bar chart\nfig = go.Figure(data=[go.Bar(\n    x=counts_list,\n    y=words_list,\n    orientation='h',\n    marker_color='#667eea',\n    text=counts_list,\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title=f'Top 20 Words in {category.capitalize()} Articles',\n    xaxis_title='Frequency',\n    yaxis_title='Words',\n    width=800,\n    height=600,\n    yaxis={'autorange': 'reversed'}  # Top word at top\n)\n\nfig.show()\n\n# Print statistics\nprint(f\"\\nTop words in {category}:\")\nfor word, count in word_counts[:10]:\n    print(f\"  {word:15s}: {count:4d}\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Analyze all categories\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Top 10 Words by Category', fontsize=16, y=0.995)\n\ncategories = sorted(df['label'].unique())\ncolors = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b']\n\nfor idx, category in enumerate(categories):\n    ax = axes[idx // 3, idx % 3]\n    \n    # Get word frequencies\n    cat_df = df[df['label'] == category]\n    all_text = ' '.join(cat_df['text']).lower()\n    import re\n    words = re.findall(r'\b[a-z]{3,}\b', all_text)\n    words = [w for w in words if w not in stop_words]\n    word_counts = Counter(words).most_common(10)\n    \n    words_list = [word for word, count in word_counts]\n    counts_list = [count for word, count in word_counts]\n    \n    # Create horizontal bar chart\n    ax.barh(words_list, counts_list, color=colors[idx], alpha=0.7)\n    ax.set_title(category.capitalize(), fontsize=12, fontweight='bold')\n    ax.set_xlabel('Frequency')\n    ax.invert_yaxis()  # Top word at top\n    ax.grid(axis='x', alpha=0.3)\n\n# Remove empty subplot\nfig.delaxes(axes[1, 2])\n\nplt.tight_layout()\nplt.show()",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Get top words for one category\ncategory = 'tech'  # Change category here\ncat_df = df[df['label'] == category]\n\n# Get word frequencies\nall_text = ' '.join(cat_df['text']).lower()\nwords = [w for w in all_text.split() if w.isalpha() and w not in stop_words]\nword_counts = Counter(words).most_common(15)\n\n# Create DataFrame for seaborn\nwords_df = pd.DataFrame(word_counts, columns=['word', 'count'])\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.barplot(data=words_df, y='word', x='count', \n           color='#43e97b', ax=ax, alpha=0.8)\n\nax.set_title(f'Top 15 Words in {category.capitalize()} Articles', \n            fontsize=14, fontweight='bold')\nax.set_xlabel('Frequency', fontsize=12)\nax.set_ylabel('Words', fontsize=12)\nax.grid(axis='x', alpha=0.3)\n\n# Add value labels\nfor i, (word, count) in enumerate(word_counts[:15]):\n    ax.text(count + 5, i, f'{count}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()"
  },
  "tfidf_terms": {
    "title": "TF-IDF Top Terms by Category",
    "explanation": {
      "what": "TF-IDF (Term Frequency-Inverse Document Frequency) finds words that are both frequent in a category AND rare in other categories. These are the most distinctive words.",
      "why": "Better than simple word frequency because it identifies category-specific terms. 'Film' has high TF-IDF in Entertainment but low in Sport, making it a good discriminator.",
      "how": "High TF-IDF = important AND unique to category. These are the best features for classification models. Compare across categories to find distinctive vocabulary."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Select category\ncategory = 'sport'  # Change to: business, entertainment, politics, sport, tech\n\n# Calculate TF-IDF on ALL documents in dataset (same as report)\nvectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\ntfidf_matrix = vectorizer.fit_transform(df['text'])\nfeature_names = vectorizer.get_feature_names_out()\n\n# Get indices of documents in selected category\ncat_indices = df[df['label'] == category].index\n\n# Get TF-IDF scores for documents in this category\ncat_tfidf = tfidf_matrix[cat_indices]\n\n# Calculate MEAN TF-IDF score for each term (same as report)\nmean_scores = np.array(cat_tfidf.mean(axis=0)).flatten()\n\n# Get top 20 terms\ntop_indices = mean_scores.argsort()[-20:][::-1]\ntop_words = [feature_names[i] for i in top_indices]\ntop_scores = [mean_scores[i] for i in top_indices]\n\n# Create horizontal bar chart\nfig = go.Figure(data=[go.Bar(\n    x=top_scores,\n    y=top_words,\n    orientation='h',\n    marker_color='#764ba2',\n    text=[f'{score:.4f}' for score in top_scores],\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title=f'Top 20 TF-IDF Terms in {category.capitalize()}',\n    xaxis_title='TF-IDF Score',\n    yaxis_title='Terms',\n    width=800,\n    height=600,\n    yaxis={'autorange': 'reversed'}\n)\n\nfig.show()\n\n# Print top terms\nprint(f\"\\nTop TF-IDF terms in {category}:\")\nfor word, score in zip(top_words[:10], top_scores[:10]):\n    print(f\"  {word:20s}: {score:.4f}\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Calculate TF-IDF on ALL documents first (same as report)\nvectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\ntfidf_matrix = vectorizer.fit_transform(df['text'])\nfeature_names = vectorizer.get_feature_names_out()\n\n# Analyze all categories\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\nfig.suptitle('Top 10 TF-IDF Terms by Category', fontsize=16, y=0.995)\n\ncategories = sorted(df['label'].unique())\ncolors = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b']\n\nfor idx, category in enumerate(categories):\n    ax = axes[idx // 3, idx % 3]\n    \n    # Get TF-IDF for this category\n    cat_indices = df[df['label'] == category].index\n    cat_tfidf = tfidf_matrix[cat_indices]\n    \n    # Calculate mean TF-IDF scores\n    mean_scores = np.array(cat_tfidf.mean(axis=0)).flatten()\n    \n    # Get top 10\n    top_indices = mean_scores.argsort()[-10:][::-1]\n    top_words = [feature_names[i] for i in top_indices]\n    top_scores = [mean_scores[i] for i in top_indices]\n    \n    # Plot\n    ax.barh(top_words, top_scores, color=colors[idx], alpha=0.7)\n    ax.set_title(category.capitalize(), fontsize=12, fontweight='bold')\n    ax.set_xlabel('TF-IDF Score', fontsize=10)\n    ax.invert_yaxis()\n    ax.grid(axis='x', alpha=0.3)\n\n# Remove empty subplot\nfig.delaxes(axes[1, 2])\n\nplt.tight_layout()\nplt.show()",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Select category\ncategory = 'entertainment'  # Change category here\n\n# Calculate TF-IDF on ALL documents (same as report)\nvectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\ntfidf_matrix = vectorizer.fit_transform(df['text'])\nfeature_names = vectorizer.get_feature_names_out()\n\n# Get indices of documents in this category\ncat_indices = df[df['label'] == category].index\ncat_tfidf = tfidf_matrix[cat_indices]\n\n# Calculate MEAN TF-IDF scores for this category\nmean_scores = np.array(cat_tfidf.mean(axis=0)).flatten()\n\n# Get top 15 terms\ntop_indices = mean_scores.argsort()[-15:][::-1]\ntfidf_df = pd.DataFrame({\n    'term': [feature_names[i] for i in top_indices],\n    'tfidf_score': [mean_scores[i] for i in top_indices]\n})\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.barplot(data=tfidf_df, y='term', x='tfidf_score', \n           color='#f093fb', ax=ax, alpha=0.8)\n\nax.set_title(f'Top 15 TF-IDF Terms in {category.capitalize()}', \n            fontsize=14, fontweight='bold')\nax.set_xlabel('TF-IDF Score', fontsize=12)\nax.set_ylabel('Terms', fontsize=12)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  "ngram_analysis": {
    "title": "N-gram Analysis (Bigrams)",
    "explanation": {
      "what": "N-grams are sequences of N consecutive words. Bigrams (2-word phrases) reveal common phrases and collocations like 'prime minister' or 'box office'.",
      "why": "Captures phrase-level patterns that single words miss. 'New York' means something different than 'new' and 'york' separately. Important for understanding context.",
      "how": "High-frequency bigrams show common phrases in each category. Use TF-IDF on bigrams to find category-specific phrases. Useful for feature engineering."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Select category\ncategory = 'politics'  # Change to: business, entertainment, politics, sport, tech\ncat_df = df[df['label'] == category]\n\n# Combine all articles in category into ONE text (same as report)\ncombined_text = ' '.join(cat_df['text'].values)\n\n# Extract bigrams using TF-IDF on the combined text\nvectorizer = TfidfVectorizer(\n    ngram_range=(2, 2),  # Bigrams only\n    max_features=50,\n    stop_words=list(stop_words)\n)\n# Fit on single combined text (returns 1 row)\ntfidf_matrix = vectorizer.fit_transform([combined_text])\n\n# Get TF-IDF scores (single row, so no need for mean)\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_scores = tfidf_matrix.toarray()[0]\n\n# Get top 15 bigrams\ntop_indices = tfidf_scores.argsort()[-15:][::-1]\ntop_bigrams = [feature_names[i] for i in top_indices]\ntop_scores = [tfidf_scores[i] for i in top_indices]\n\n# Create horizontal bar chart\nfig = go.Figure(data=[go.Bar(\n    x=top_scores,\n    y=top_bigrams,\n    orientation='h',\n    marker_color='#4facfe',\n    text=[f'{score:.4f}' for score in top_scores],\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title=f'Top 15 Bigrams in {category.capitalize()} (TF-IDF)',\n    xaxis_title='TF-IDF Score',\n    yaxis_title='Bigrams',\n    width=900,\n    height=600,\n    yaxis={'autorange': 'reversed'}\n)\n\nfig.show()\n\n# Print results\nprint(f\"\\nTop bigrams in {category}:\")\nfor bigram, score in zip(top_bigrams[:10], top_scores[:10]):\n    print(f\"  {bigram:25s}: {score:.4f}\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Analyze all categories\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\nfig.suptitle('Top 10 Bigrams by Category (TF-IDF)', fontsize=16, y=0.995)\n\ncategories = sorted(df['label'].unique())\ncolors = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b']\n\nfor idx, category in enumerate(categories):\n    ax = axes[idx // 3, idx % 3]\n    \n    # Extract bigrams for this category\n    cat_df = df[df['label'] == category]\n    # Combine all articles into one text (same as report)\n    combined_text = ' '.join(cat_df['text'].values)\n    \n    vectorizer = TfidfVectorizer(\n        ngram_range=(2, 2),\n        max_features=30,\n        stop_words=list(stop_words)\n    )\n    # Fit on single combined text\n    tfidf_matrix = vectorizer.fit_transform([combined_text])\n    \n    # Get scores (single row)\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_scores = tfidf_matrix.toarray()[0]\n    \n    # Get top 10\n    top_indices = tfidf_scores.argsort()[-10:][::-1]\n    top_bigrams = [feature_names[i] for i in top_indices]\n    top_scores = [tfidf_scores[i] for i in top_indices]\n    \n    # Plot\n    ax.barh(top_bigrams, top_scores, color=colors[idx], alpha=0.7)\n    ax.set_title(category.capitalize(), fontsize=12, fontweight='bold')\n    ax.set_xlabel('TF-IDF Score', fontsize=9)\n    ax.invert_yaxis()\n    ax.grid(axis='x', alpha=0.3)\n    ax.tick_params(axis='y', labelsize=8)\n\n# Remove empty subplot\nfig.delaxes(axes[1, 2])\n\nplt.tight_layout()\nplt.show()",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Select category\ncategory = 'sport'  # Change category here\ncat_df = df[df['label'] == category]\n\n# Combine all articles into ONE text (same as report)\ncombined_text = ' '.join(cat_df['text'].values)\n\n# Extract bigrams using TF-IDF on combined text\nvectorizer = TfidfVectorizer(\n    ngram_range=(2, 2),  # Bigrams only\n    max_features=50,\n    stop_words=list(stop_words)\n)\n# Fit on single combined text\ntfidf_matrix = vectorizer.fit_transform([combined_text])\n\n# Get TF-IDF scores (single row)\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_scores = tfidf_matrix.toarray()[0]\n\n# Get top 15 bigrams\ntop_indices = tfidf_scores.argsort()[-15:][::-1]\nbigram_df = pd.DataFrame({\n    'bigram': [feature_names[i] for i in top_indices],\n    'tfidf_score': [tfidf_scores[i] for i in top_indices]\n})\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.barplot(data=bigram_df, y='bigram', x='tfidf_score', \n           color='#4facfe', ax=ax, alpha=0.8)\n\nax.set_title(f'Top 15 Bigrams in {category.capitalize()}', \n            fontsize=14, fontweight='bold')\nax.set_xlabel('TF-IDF Score', fontsize=12)\nax.set_ylabel('Bigrams', fontsize=12)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  "category_similarity": {
    "title": "Category Similarity Matrix",
    "explanation": {
      "what": "Cosine similarity matrix shows how similar categories are based on TF-IDF vectors. Computed as MEAN similarity across all document pairs between categories.",
      "why": "Predicts classification difficulty. High similarity means categories use similar vocabulary, making them harder for ML models to distinguish.",
      "how": "Diagonal shows average within-category similarity (typically 0.04-0.07). Off-diagonal shows cross-category similarity. Lower values = easier to distinguish. Values > 0.02 between different categories may indicate confusion."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Calculate TF-IDF on ALL documents (not combined per category)\ncategories = sorted(df['label'].unique())\nvectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\ntfidf_matrix = vectorizer.fit_transform(df['text'])\n\n# Calculate mean similarity between each pair of categories\nn_cats = len(categories)\nsimilarity_matrix = np.zeros((n_cats, n_cats))\n\nfor i, cat1 in enumerate(categories):\n    cat1_indices = df[df['label'] == cat1].index\n    cat1_vectors = tfidf_matrix[cat1_indices]\n    \n    for j, cat2 in enumerate(categories):\n        cat2_indices = df[df['label'] == cat2].index\n        cat2_vectors = tfidf_matrix[cat2_indices]\n        \n        # Compute pairwise similarities between all document pairs\n        pairwise_sim = cosine_similarity(cat1_vectors, cat2_vectors)\n        \n        # Take mean similarity (this is why diagonal != 1.0)\n        similarity_matrix[i, j] = np.mean(pairwise_sim)\n\n# Create heatmap\nfig = go.Figure(data=go.Heatmap(\n    z=similarity_matrix,\n    x=categories,\n    y=categories,\n    colorscale='Purples',\n    text=similarity_matrix,\n    texttemplate='%{text:.3f}',\n    textfont={\"size\": 12},\n    colorbar=dict(title=\"Similarity\")\n))\n\nfig.update_layout(\n    title='Category Similarity Matrix (Cosine Similarity)',\n    xaxis_title='Category',\n    yaxis_title='Category',\n    width=700,\n    height=600,\n    yaxis={'autorange': 'reversed'}\n)\n\nfig.show()\n\n# Print similarity matrix\nprint(\"\\nCategory Similarity Matrix:\")\nprint(\"Note: Diagonal is NOT 1.0 because it's MEAN similarity between documents\")\nprint(\"within same category, not self-similarity\\n\")\n\nfor i, cat1 in enumerate(categories):\n    print(f\"{cat1:15s}: {similarity_matrix[i][i]:.3f} (within-category)\")\n    for j, cat2 in enumerate(categories):\n        if i < j:  # Cross-category only\n            sim = similarity_matrix[i][j]\n            status = \"⚠️ High\" if sim > 0.02 else \"✓ Low\"\n            print(f\"  vs {cat2:15s}: {sim:.3f} {status}\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Calculate TF-IDF on ALL documents\ncategories = sorted(df['label'].unique())\nvectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\ntfidf_matrix = vectorizer.fit_transform(df['text'])\n\n# Calculate mean similarity between each pair of categories\nn_cats = len(categories)\nsimilarity_matrix = np.zeros((n_cats, n_cats))\n\nfor i, cat1 in enumerate(categories):\n    cat1_indices = df[df['label'] == cat1].index\n    cat1_vectors = tfidf_matrix[cat1_indices]\n    \n    for j, cat2 in enumerate(categories):\n        cat2_indices = df[df['label'] == cat2].index\n        cat2_vectors = tfidf_matrix[cat2_indices]\n        \n        # Compute pairwise similarities\n        pairwise_sim = cosine_similarity(cat1_vectors, cat2_vectors)\n        \n        # Take mean (diagonal != 1.0)\n        similarity_matrix[i, j] = np.mean(pairwise_sim)\n\n# Create heatmap\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(similarity_matrix, cmap='Purples', aspect='auto')\n\n# Set ticks and labels\nax.set_xticks(np.arange(len(categories)))\nax.set_yticks(np.arange(len(categories)))\nax.set_xticklabels(categories)\nax.set_yticklabels(categories)\n\n# Rotate x labels\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n\n# Add colorbar\ncbar = plt.colorbar(im, ax=ax)\ncbar.set_label('Similarity Score', rotation=270, labelpad=20)\n\n# Add text annotations\nfor i in range(len(categories)):\n    for j in range(len(categories)):\n        text = ax.text(j, i, f'{similarity_matrix[i, j]:.3f}',\n                      ha=\"center\", va=\"center\", \n                      color=\"white\" if similarity_matrix[i, j] > 0.5 else \"black\",\n                      fontsize=10, fontweight='bold')\n\nax.set_title('Category Similarity Matrix', fontsize=16, pad=20)\nax.set_xlabel('Category', fontsize=12)\nax.set_ylabel('Category', fontsize=12)\n\nplt.tight_layout()\nplt.show()",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Setup stopwords (same as web report)\nstop_words = set(ENGLISH_STOP_WORDS)\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Calculate TF-IDF on ALL documents (not combined)\ncategories = sorted(df['label'].unique())\nvectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\ntfidf_matrix = vectorizer.fit_transform(df['text'])\n\n# Calculate mean similarity between each pair of categories\nn_cats = len(categories)\nsimilarity_matrix = np.zeros((n_cats, n_cats))\n\nfor i, cat1 in enumerate(categories):\n    cat1_indices = df[df['label'] == cat1].index\n    cat1_vectors = tfidf_matrix[cat1_indices]\n    \n    for j, cat2 in enumerate(categories):\n        cat2_indices = df[df['label'] == cat2].index\n        cat2_vectors = tfidf_matrix[cat2_indices]\n        \n        # Pairwise similarities between all document pairs\n        pairwise_sim = cosine_similarity(cat1_vectors, cat2_vectors)\n        \n        # Mean similarity (diagonal is mean within-category similarity, not 1.0)\n        similarity_matrix[i, j] = np.mean(pairwise_sim)\n\n# Create DataFrame for better labels\nsimilarity_df = pd.DataFrame(\n    similarity_matrix,\n    index=categories,\n    columns=categories\n)\n\n# Create heatmap with seaborn\nfig, ax = plt.subplots(figsize=(10, 8))\n\nsns.heatmap(similarity_df, \n           annot=True,          # Show values\n           fmt='.3f',           # 3 decimal places\n           cmap='Purples',      # Color scheme\n           square=True,         # Square cells\n           linewidths=0.5,      # Cell borders\n           cbar_kws={'label': 'Similarity Score'},\n           ax=ax)\n\nax.set_title('Category Similarity Matrix', fontsize=16, pad=20)\nax.set_xlabel('Category', fontsize=12)\nax.set_ylabel('Category', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\n# Print interpretation\nprint(\"\\nInterpretation (for MEAN pairwise similarity):\")\nprint(\"  Diagonal (0.04-0.07): Within-category similarity\")\nprint(\"  Off-diagonal:\")\nprint(\"    0.00-0.015: Very low (excellent separation)\")\nprint(\"    0.015-0.025: Low (good separation)\")\nprint(\"    0.025+: Higher (potential confusion)\")\nprint(\"\\nNote: These are MUCH lower than document-to-document similarity\")\nprint(\"because we average across many document pairs!\")"
  },
  "word_frequency": {
    "title": "Most Frequent Words (Overall)",
    "explanation": {
      "what": "Shows the most common words across ALL categories after removing stop words. These are the general terms that appear frequently throughout the entire dataset.",
      "why": "Helps understand the overall vocabulary and common themes across all news articles. Words appearing here are general terms, not category-specific.",
      "how": "Compare with category-specific keywords. If a word appears in overall frequency AND in a category's top words, it's common across dataset. Category-specific words appear in category analysis but not here."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Download stop words\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Get all words (remove stop words)\nimport re\n\nall_text = ' '.join(df['text']).lower()\n# Use regex to match words >= 3 characters (same as report)\nwords = re.findall(r'\b[a-z]{3,}\b', all_text)\nwords = [w for w in words if w not in stop_words]\nword_counts = Counter(words).most_common(25)\n\nwords_list = [word for word, count in word_counts]\ncounts_list = [count for word, count in word_counts]\n\n# Create bar chart\nfig = go.Figure(data=[go.Bar(\n    x=words_list,\n    y=counts_list,\n    marker_color='#43e97b',\n    text=counts_list,\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title='Top 25 Most Frequent Words (Overall)',\n    xaxis_title='Words',\n    yaxis_title='Frequency',\n    width=900,\n    height=500,\n    xaxis={'tickangle': 45}\n)\n\nfig.show()\n\n# Print statistics\nprint(\"\\nTop 20 words across all categories:\")\nfor word, count in word_counts[:20]:\n    print(f\"  {word:15s}: {count:5d}\")\n    \nprint(f\"\\nTotal unique words (no stop words): {len(set(words)):,}\")\nprint(f\"Total words (no stop words): {len(words):,}\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Get all words (remove stop words)\nall_text = ' '.join(df['text']).lower()\nwords = [w for w in all_text.split() if w.isalpha() and w not in stop_words]\nword_counts = Counter(words).most_common(20)\n\nwords_list = [word for word, count in word_counts]\ncounts_list = [count for word, count in word_counts]\n\n# Create bar chart\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.bar(words_list, counts_list, color='#43e97b', alpha=0.7, edgecolor='black')\n\nax.set_title('Top 20 Most Frequent Words (Overall)', fontsize=16, pad=20)\nax.set_xlabel('Words', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor i, (word, count) in enumerate(word_counts):\n    ax.text(i, count + 30, f'{count}', ha='center', fontsize=9)\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(f\"Total unique words: {len(set(words)):,}\")\nprint(f\"Total words: {len(words):,}\")",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n# No need for nltk - using sklearn stop words\n\n# Use sklearn stop words + custom news stopwords (same as report)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstop_words = set(ENGLISH_STOP_WORDS)\n# Add custom news stopwords\ncustom_news_stopwords = ['said', 'mr', 'ms', 'mrs', 'told', 'says', 'say', \n                         'according', 'year', 'years', 'new', 'old', 'like', \n                         'just', 'going', 'got', 'use', 'used', 'make', 'made']\nstop_words.update(custom_news_stopwords)\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/bbcnews/data/bbc-news.csv'\ndf = pd.read_csv(url)\n\n# Get all words (remove stop words)\nall_text = ' '.join(df['text']).lower()\nwords = [w for w in all_text.split() if w.isalpha() and w not in stop_words]\nword_counts = Counter(words).most_common(20)\n\n# Create DataFrame\nwords_df = pd.DataFrame(word_counts, columns=['word', 'count'])\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsns.barplot(data=words_df, x='word', y='count', \n           color='#43e97b', ax=ax, alpha=0.8)\n\nax.set_title('Top 20 Most Frequent Words (Overall)', \n            fontsize=16, pad=20)\nax.set_xlabel('Words', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels\nfor i, row in words_df.iterrows():\n    ax.text(i, row['count'] + 30, f\"{row['count']}\", \n           ha='center', fontsize=9)\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
  }
}