{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BBC News Text Classification\n",
        "## TF-IDF + Traditional Machine Learning\n",
        "\n",
        "**Dataset:** BBC News (2225 articles, 5 categories)\n",
        "\n",
        "**Methods:** Naive Bayes, Logistic Regression, SVM, Random Forest, MLP, XGBoost\n",
        "\n",
        "**Source:** [AI Learning Hub](https://ltsach.github.io/AILearningHub/04_Natural_Language_Processing/text_classification/)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q plotly xgboost\n",
        "print('\u2705 Dependencies installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "print('\u2705 Libraries imported')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Dataset\n",
        "BBC News dataset hosted on GitHub Pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_url = 'https://ltsach.github.io/AILearningHub/datasets/bbcnews/data/'\n",
        "\n",
        "train_df = pd.read_csv(base_url + 'train.csv')\n",
        "val_df = pd.read_csv(base_url + 'val.csv')\n",
        "test_df = pd.read_csv(base_url + 'test.csv')\n",
        "\n",
        "print(f'\u2713 Train: {len(train_df):,} samples')\n",
        "print(f'\u2713 Val: {len(val_df):,} samples')\n",
        "print(f'\u2713 Test: {len(test_df):,} samples')\n",
        "print(f'\u2713 Categories: {sorted(train_df[\"category\"].unique().tolist())}')\n",
        "\n",
        "# Combine train + val\n",
        "train_full = pd.concat([train_df, val_df], ignore_index=True)\n",
        "print(f'\\n\u2713 Combined training set: {len(train_full):,} samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. TF-IDF Feature Extraction\n",
        "Convert text to numerical features using Term Frequency - Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),  # Unigrams + bigrams\n",
        "    min_df=2,\n",
        "    max_df=0.8,\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "X_train = vectorizer.fit_transform(train_full['text'])\n",
        "X_test = vectorizer.transform(test_df['text'])\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f'\u2713 Vocabulary: {len(vectorizer.get_feature_names_out()):,} features')\n",
        "print(f'\u2713 Train shape: {X_train.shape}')\n",
        "print(f'\u2713 Test shape: {X_test.shape}')\n",
        "print(f'\u2713 Extraction time: {elapsed:.2f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode labels for XGBoost compatibility\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(train_full['category'])\n",
        "y_test_encoded = label_encoder.transform(test_df['category'])\n",
        "\n",
        "y_train = train_full['category']\n",
        "y_test = test_df['category']\n",
        "\n",
        "print(f'\u2713 Label encoding: {len(label_encoder.classes_)} classes')\n",
        "print(f'  Classes: {label_encoder.classes_}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier(name, model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate a classifier\"\"\"\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'{name.upper()}')\n",
        "    print('='*60)\n",
        "    \n",
        "    # Train\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start\n",
        "    \n",
        "    # Predict\n",
        "    start = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    inference_time = time.time() - start\n",
        "    \n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    print(f'\u23f1\ufe0f  Training: {train_time:.2f}s')\n",
        "    print(f'\u23f1\ufe0f  Inference: {inference_time:.3f}s')\n",
        "    print(f'\ud83d\udcca Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)')\n",
        "    print(f'\ud83d\udcca Precision: {precision:.4f} ({precision*100:.2f}%)')\n",
        "    print(f'\ud83d\udcca Recall: {recall:.4f} ({recall*100:.2f}%)')\n",
        "    print(f'\ud83d\udcca F1-Score: {f1:.4f} ({f1*100:.2f}%)')\n",
        "    \n",
        "    return {\n",
        "        'name': name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'train_time': train_time,\n",
        "        'inference_speed': len(y_test) / inference_time if inference_time > 0 else 0,\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train All Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# 1. Naive Bayes\n",
        "r = train_classifier('Naive Bayes', MultinomialNB(), X_train, y_train, X_test, y_test)\n",
        "results.append(r)\n",
        "\n",
        "# 2. Logistic Regression\n",
        "r = train_classifier('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42), X_train, y_train, X_test, y_test)\n",
        "results.append(r)\n",
        "\n",
        "# 3. SVM\n",
        "r = train_classifier('SVM', LinearSVC(max_iter=2000, random_state=42), X_train, y_train, X_test, y_test)\n",
        "results.append(r)\n",
        "\n",
        "# 4. Random Forest\n",
        "r = train_classifier('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1), X_train, y_train, X_test, y_test)\n",
        "results.append(r)\n",
        "\n",
        "# 5. MLP\n",
        "r = train_classifier('MLP', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42), X_train, y_train, X_test, y_test)\n",
        "results.append(r)\n",
        "\n",
        "# 6. XGBoost (with encoded labels)\n",
        "r = train_classifier('XGBoost', XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1, verbosity=0), X_train, y_train_encoded, X_test, y_test_encoded)\n",
        "results.append(r)\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('\u2705 ALL TRAINING COMPLETE!')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Method': r['name'],\n",
        "    'Accuracy (%)': f\"{r['accuracy']*100:.2f}\",\n",
        "    'Precision (%)': f\"{r['precision']*100:.2f}\",\n",
        "    'Recall (%)': f\"{r['recall']*100:.2f}\",\n",
        "    'F1-Score (%)': f\"{r['f1_score']*100:.2f}\",\n",
        "    'Train Time (s)': f\"{r['train_time']:.2f}\",\n",
        "    'Inference Speed (samples/s)': f\"{r['inference_speed']:.0f}\"\n",
        "} for r in results])\n",
        "\n",
        "print(summary_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Interactive Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison chart\n",
        "names = [r['name'] for r in results]\n",
        "accuracies = [r['accuracy']*100 for r in results]\n",
        "train_times = [r['train_time'] for r in results]\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=('Accuracy Comparison', 'Training Time Comparison')\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(x=names, y=accuracies, name='Accuracy (%)', marker_color='rgb(102, 126, 234)'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(x=names, y=train_times, name='Time (s)', marker_color='rgb(245, 135, 108)'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(height=400, showlegend=False, title_text='Performance Comparison')\n",
        "fig.update_yaxes(title_text='Accuracy (%)', row=1, col=1)\n",
        "fig.update_yaxes(title_text='Time (seconds)', row=1, col=2)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusion\n",
        "\n",
        "**Best Models:**\n",
        "- **Accuracy:** Logistic Regression & MLP (98.8%)\n",
        "- **Speed:** Naive Bayes (0.01s training)\n",
        "- **Balance:** Logistic Regression (fast + accurate)\n",
        "\n",
        "**Key Insights:**\n",
        "- TF-IDF + simple classifiers achieve 96-99% accuracy\n",
        "- Training time: 0.01s (Naive Bayes) to 8s (MLP)\n",
        "- All models are very fast at inference (<1ms per sample)\n",
        "\n",
        "**For more:**\n",
        "- [Interactive Report](https://ltsach.github.io/AILearningHub/04_Natural_Language_Processing/text_classification/)\n",
        "- [Download Code](https://ltsach.github.io/AILearningHub/04_Natural_Language_Processing/text_classification/pages/traditional_ml/code/bbc_news_tfidf_ml.py)"
      ]
    }
  ]
}