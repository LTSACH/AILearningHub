<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CrossEntropy Loss Function - AI Learning Hub</title>

  <!-- MathJax Configuration -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Prism.js for syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6; color: #333;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
    }
    .container { max-width: 1200px; margin: 0 auto; padding: 0 20px; }
    header {
      background: rgba(255, 255, 255, 0.95); backdrop-filter: blur(10px);
      box-shadow: 0 2px 20px rgba(0,0,0,0.1); position: sticky; top: 0; z-index: 1000;
    }
    nav { display: flex; justify-content: space-between; align-items: center; padding: 1rem 0; }
    .logo { font-size: 1.5rem; font-weight: bold; color: #667eea; text-decoration: none; }
    .breadcrumb { font-size: 0.9rem; color: #666; }
    .breadcrumb a { color: #667eea; text-decoration: none; }
    .breadcrumb a:hover { text-decoration: underline; }
    main { padding: 2rem 0; }
    .content-card {
      background: rgba(255,255,255,0.95);
      border-radius: 15px; padding: 2rem; margin-bottom: 2rem;
      box-shadow: 0 8px 32px rgba(0,0,0,0.1); backdrop-filter: blur(10px);
    }
    h1 { color: #667eea; margin-bottom: 1rem; font-size: 2.5rem; }
    h2 {
      color: #667eea; margin: 2rem 0 1rem 0; font-size: 1.8rem;
      border-bottom: 2px solid #667eea; padding-bottom: 0.5rem;
    }
    h3 { color: #555; margin: 1.5rem 0 0.5rem 0; font-size: 1.3rem; }
    .overview {
      background: linear-gradient(135deg, #ff6b6b, #ee5a24);
      color: white; border-radius: 10px; padding: 1.5rem; margin: 1rem 0;
    }
    .overview h3 { color: white; margin-top: 0; }
    .math-display {
      margin: 1.5rem 0; text-align: center; font-size: 1.2rem;
    }
    .math-inline {
      font-size: 1rem;
    }
    .learning-objectives {
      background: linear-gradient(135deg, #4ecdc4, #44a08d);
      color: white; border-radius: 10px; padding: 1.5rem; margin: 1rem 0;
    }
    .learning-objectives h3 { color: white; margin-top: 0; }
    .learning-objectives ul { list-style: none; padding-left: 0; }
    .learning-objectives li { margin: 0.5rem 0; padding-left: 1.5rem; position: relative; }
    .learning-objectives li::before {
      content: "‚úì"; position: absolute; left: 0; color: #fff; font-weight: bold;
    }
    .estimated-time {
      background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px;
      padding: 1rem; margin: 1rem 0; color: #856404;
    }
    .estimated-time strong { color: #667eea; }
    .back-button {
      display: inline-block; background: linear-gradient(45deg, #667eea, #764ba2);
      color: white; padding: 0.8rem 1.5rem; text-decoration: none; border-radius: 25px;
      font-weight: bold; transition: transform 0.3s ease, box-shadow 0.3s ease;
      box-shadow: 0 4px 15px rgba(102,126,234,0.4); margin-bottom: 2rem;
    }
    .back-button:hover { transform: translateY(-2px); box-shadow: 0 6px 20px rgba(102,126,234,0.6); }
    table {
      width: 100%; border-collapse: collapse; margin: 1rem 0; background: white;
      border-radius: 8px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    thead tr { background: #667eea; color: white; }
    th, td { padding: 12px; }
    th { text-align: left; }
    td.center { text-align: center; }
    @media (max-width: 768px) {
      h1 { font-size: 2rem; }
      .section-nav { grid-template-columns: 1fr; }
      .content-card { padding: 1.5rem; }
    }
    .interpretation-list {
      margin-left: 1rem;
      padding-left: 1rem;
    }
    .code-container {
      position: relative;
      margin: 1rem 0;
    }
    .code-header {
      background: #667eea;
      color: white;
      padding: 0.5rem 1rem;
      border-radius: 8px 8px 0 0;
      font-weight: bold;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    .copy-btn {
      background: rgba(255, 255, 255, 0.2);
      border: 1px solid rgba(255, 255, 255, 0.3);
      color: white;
      padding: 0.25rem 0.75rem;
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.8rem;
      transition: all 0.3s ease;
    }
    .copy-btn:hover {
      background: rgba(255, 255, 255, 0.3);
    }
    .copy-btn.copied {
      background: #28a745;
      border-color: #28a745;
    }
    pre[class*="language-"] {
      margin: 0;
      border-radius: 0 0 8px 8px;
      border: 1px solid #e9ecef;
      border-top: none;
    }
    .code-section {
      margin: 2rem 0;
    }
    .code-section h3 {
      color: #667eea;
      margin-bottom: 1rem;
      font-size: 1.3rem;
    }
    .code-section p {
      margin-bottom: 1rem;
      color: #666;
    }
    .modal-open {
      overflow: hidden !important;
    }
    .section-nav {
      display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 1rem; margin: 2rem 0;
    }
    .section-card {
      background: #f8f9fa; border-radius: 10px; padding: 1.5rem; text-align: center;
      transition: transform 0.3s ease, box-shadow 0.3s ease; cursor: pointer;
    }
    .section-card:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
    .section-card h4 { color: #667eea; margin-bottom: 0.5rem; }
    .section-card p { color: #666; font-size: 0.9rem; }
  </style>
</head>
<body>
<header>
  <nav class="container">
    <a href="/AILearningHub/" class="logo">ü§ñ AI Learning Hub</a>
    <div class="breadcrumb">
      <a href="/AILearningHub/">Home</a> /
      <a href="/AILearningHub/02_Machine_Learning/">Machine Learning</a> /
      CrossEntropy Loss
    </div>
  </nav>
</header>

<main>
  <div class="container">
    <a href="/AILearningHub/" class="back-button">‚Üê Back to AI Learning Hub</a>

    <div class="content-card">
      <h1>CrossEntropy Loss Function</h1>

      <div class="overview">
        <h3>üìã Overview</h3>
        <p>CrossEntropy is a fundamental loss function for classification. This page covers the mathematical foundation (MLE & information theory), gradients, and worked examples.</p>
      </div>

      <div class="learning-objectives">
        <h3>üéØ Learning Objectives</h3>
        <ul>
          <li>Understand why CrossEntropy arises from MLE</li>
          <li>Use CrossEntropy consistently with hard and soft labels</li>
           <li>Recall the key gradient result <span class="math-inline">$\frac{\partial \mathcal{L}}{\partial z} = \hat{y} - y$</span></li>
          <li>Apply CrossEntropy to real classification problems</li>
        </ul>
      </div>

      <div class="estimated-time">
        <strong>‚è±Ô∏è Estimated Time:</strong> 15‚Äì20 minutes reading + 30 minutes practice
      </div>

      <h2>Mathematical Foundation</h2>
      <p>CrossEntropy measures the discrepancy between the <span class="math-inline">$y$</span> (target distribution) and <span class="math-inline">$\hat{y}$</span> (predicted distribution).</p>

       <div class="math-display">
         <strong>Binary CrossEntropy (Bernoulli):</strong><br><br>
         $$\mathcal{L}(y,\hat{y}) = -\big[ y\,\log\hat{y} + (1-y)\,\log(1-\hat{y}) \big],\quad y\in[0,1]$$
       </div>

       <div class="math-display">
         <strong>Multi-class CrossEntropy (Categorical):</strong><br><br>
         $$\mathcal{L}(y, \hat{y}) = - \sum_{i=1}^{C} y_i\,\log \hat{y}_i$$
         <p>where $y$ is the target distribution (hard or soft) and $\hat{y}$ is the predicted distribution (e.g., softmax output).</p>
       </div>

      <h2>Theoretical Foundations</h2>

      <h3>1. Maximum Likelihood Estimation (MLE) Perspective</h3>
      <p>Assume we have $N$ samples. The target for sample $n$ is a distribution $y^{(n)} = [y^{(n)}_1,\ldots,y^{(n)}_C]$ (hard one-hot or soft). The model predicts $\hat{y}^{(n)} = [\hat{y}^{(n)}_1,\ldots,\hat{y}^{(n)}_C]$ with $\sum_i \hat{y}^{(n)}_i = 1$.</p>

      <div class="math-display">
        <strong>Likelihood:</strong><br><br>
        $$L(\theta) = \prod_{n=1}^N \prod_{i=1}^C \big(\hat{y}^{(n)}_i\big)^{\,y^{(n)}_i}$$
      </div>

      <p>Taking logs:</p>
      <div class="math-display">
        $$\log L(\theta) = \sum_{n=1}^N \sum_{i=1}^C y^{(n)}_i \log \hat{y}^{(n)}_i.$$
      </div>

      <p>Minimizing the negative log-likelihood (NLL) gives exactly the CrossEntropy:</p>
      <div class="math-display">
        $$\mathcal{L}_{\text{NLL}}(\theta) = -\log L(\theta) = -\sum_{n=1}^N \sum_{i=1}^C y^{(n)}_i \log \hat{y}^{(n)}_i.$$
      </div>

      <p><strong>Key insight:</strong> CrossEntropy is the negative log-likelihood whether $y$ is one-hot (hard) or a soft distribution.</p>

      <h3>2. Information Theory Perspective</h3>
      <p>
        In information theory, <strong>CrossEntropy</strong> 
        <span class="math-inline">$H(y,\hat{y})$</span> represents the expected code length
        for data sampled from the true distribution 
        <span class="math-inline">$y$</span> when using a code optimized for the 
        predicted distribution <span class="math-inline">$\hat{y}$</span>.
      </p>

      <div class="math-display">
        $$H(y,\hat{y}) = -\sum_{i=1}^{C} y_i \log \hat{y}_i$$
      </div>

      <p>
        CrossEntropy is related to the entropy of the true distribution 
        and the KL divergence as:
      </p>

      <div class="math-display">
        $$H(y,\hat{y}) = H(y) + D_{\mathrm{KL}}(y \,\|\, \hat{y}).$$
      </div>

      <p>
        Here:
      </p>

      <div class="math-display">
        $$H(y) = -\sum_{i=1}^{C} y_i \log y_i \quad \text{(Entropy)}$$
      </div>

      <div class="math-display">
        $$D_{\mathrm{KL}}(y \,\|\, \hat{y}) 
          = \sum_{i=1}^{C} y_i \log \frac{y_i}{\hat{y}_i} 
          = \sum_{i=1}^{C} y_i \log y_i - \sum_{i=1}^{C} y_i \log \hat{y}_i 
          = -H(y) + H(y,\hat{y}).$$
      </div>

        <strong>Interpretation:</strong>
        <ul class="interpretation-list">
          <li><span class="math-inline">$H(y,\hat{y})$</span> is the cross-entropy between the true and predicted distributions.</li>
          <li><span class="math-inline">$H(y)$</span> measures the inherent uncertainty of the true distribution.</li>
          <li><span class="math-inline">$D_{\mathrm{KL}}(y \,\|\, \hat{y}) \geq 0$</span> and equals zero iff 
              <span class="math-inline">$y = \hat{y}$</span>.</li>
              It quantifies the inefficiency (extra code length) incurred when 
              using <span class="math-inline">$\hat{y}$</span> instead of 
              <span class="math-inline">$y$</span>.
          <li>Since <span class="math-inline">$H(y)$</span> is constant w.r.t. model 
              parameters, minimizing CrossEntropy is exactly equivalent to minimizing 
              KL divergence.</li>
        </ul>

      <h3>3. Gradient Analysis</h3>
      <p>Let $\hat{y} = \mathrm{softmax}(z)$. The gradient of CrossEntropy w.r.t. the logits $z$ is:</p>
      <div class="math-display">
        $$\frac{\partial \mathcal{L}}{\partial z_i} = \hat{y}_i - y_i.$$
      </div>
      <p>This holds for both hard and soft labels.</p>

      <h2>Understanding Hard vs. Soft Labels</h2>

      <h3>Hard Labels (one-hot)</h3>
      <div class="math-display">
        For 3 classes: <br><br>
        Class 1: $y = [1, 0, 0]$ &nbsp;&nbsp;
        Class 2: $y = [0, 1, 0]$ &nbsp;&nbsp;
        Class 3: $y = [0, 0, 1]$
      </div>

      <h3>Soft Labels (probability distributions)</h3>
      <div class="math-display">
        Examples: <br><br>
        $y = [0.7, 0.2, 0.1]$, &nbsp; $y = [0.4, 0.3, 0.3]$, &nbsp; $y = [0.95, 0.03, 0.02]$
      </div>

      <h2>Applications in Classification</h2>
      <ul>
        <li><strong>Image:</strong> object recognition, handwritten digits</li>
        <li><strong>Text:</strong> sentiment analysis, spam detection</li>
        <li><strong>Medical:</strong> diagnosis from images/symptoms</li>
        <li><strong>Recommenders:</strong> preference/category prediction</li>
        <li><strong>NLP:</strong> NER, POS tagging</li>
      </ul>

      <h2>Why CrossEntropy?</h2>
      <ul>
        <li><strong>Probabilistic:</strong> directly compares distributions $y$ vs. $\hat{y}$</li>
        <li><strong>Useful gradients:</strong> strong updates when predictions are wrong</li>
        <li><strong>Convex for logistic regression</strong></li>
        <li><strong>Information-theoretic meaning</strong></li>
      </ul>
    </div>

    <div class="section-nav">
      <div class="section-card">
        <h4>üíª Code Examples</h4>
        <p>NumPy, PyTorch, TensorFlow implementations</p>
      </div>
      <div class="section-card">
        <h4>üìä Visualizations</h4>
        <p>Loss curves and gradient behavior</p>
      </div>
      <div class="section-card">
        <h4>üèãÔ∏è Exercises</h4>
        <p>Hands-on practice problems</p>
      </div>
    </div>

    <div class="content-card">
      <h2>Detailed Calculation Examples</h2>
      <p>Worked examples to see how CrossEntropy behaves.</p>

      <h3>Hard Label Examples</h3>
      <table>
        <thead>
          <tr>
            <th>Sample</th>
            <th class="center">True Label (y)</th>
             <th class="center">Prediction ($\hat{y}$)</th>
            <th class="center">CrossEntropy</th>
            <th class="center">Interpretation</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom:1px solid #eee;">
            <td><strong>1</strong></td>
            <td class="center"><span class="math-inline">$[1, 0, 0]$</span></td>
            <td class="center"><span class="math-inline">$[0.8, 0.1, 0.1]$</span></td>
            <td class="center"><span class="math-inline">$0.223$</span></td>
            <td class="center" style="color:#28a745;">Good prediction</td>
          </tr>
          <tr style="border-bottom:1px solid #eee;">
            <td><strong>2</strong></td>
            <td class="center"><span class="math-inline">$[0, 1, 0]$</span></td>
            <td class="center"><span class="math-inline">$[0.3, 0.4, 0.3]$</span></td>
            <td class="center"><span class="math-inline">$0.916$</span></td>
            <td class="center" style="color:#ffc107;">Uncertain prediction</td>
          </tr>
          <tr style="border-bottom:1px solid #eee;">
            <td><strong>3</strong></td>
            <td class="center"><span class="math-inline">$[0, 0, 1]$</span></td>
            <td class="center"><span class="math-inline">$[0.9, 0.05, 0.05]$</span></td>
            <td class="center"><span class="math-inline">$2.996$</span></td>
            <td class="center" style="color:#dc3545;">Wrong prediction</td>
          </tr>
          <tr style="border-bottom:1px solid #eee;">
            <td><strong>4</strong></td>
            <td class="center"><span class="math-inline">$[1, 0, 0]$</span></td>
            <td class="center"><span class="math-inline">$[0.99, 0.005, 0.005]$</span></td>
            <td class="center"><span class="math-inline">$0.010$</span></td>
            <td class="center" style="color:#28a745;">Excellent prediction</td>
          </tr>
          <tr>
            <td><strong>5</strong></td>
            <td class="center"><span class="math-inline">$[0, 1, 0]$</span></td>
            <td class="center"><span class="math-inline">$[0.33, 0.33, 0.34]$</span></td>
            <td class="center"><span class="math-inline">$1.099$</span></td>
            <td class="center" style="color:#ffc107;">Random prediction</td>
          </tr>
        </tbody>
      </table>

      <h3>Soft Label Examples</h3>
      <table>
        <thead>
          <tr>
            <th>Sample</th>
            <th class="center">True Label (y)</th>
             <th class="center">Prediction ($\hat{y}$)</th>
            <th class="center">CrossEntropy</th>
            <th class="center">Interpretation</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom:1px solid #eee;">
            <td><strong>6</strong></td>
            <td class="center"><span class="math-inline">$[0.7, 0.2, 0.1]$</span></td>
            <td class="center"><span class="math-inline">$[0.6, 0.3, 0.1]$</span></td>
            <td class="center"><span class="math-inline">$0.829$</span></td>
            <td class="center" style="color:#28a745;">Close match</td>
          </tr>
          <tr style="border-bottom:1px solid #eee;">
            <td><strong>7</strong></td>
            <td class="center"><span class="math-inline">$[0.4, 0.3, 0.3]$</span></td>
            <td class="center"><span class="math-inline">$[0.8, 0.1, 0.1]$</span></td>
            <td class="center"><span class="math-inline">$0.891$</span></td>
            <td class="center" style="color:#ffc107;">Overconfident</td>
          </tr>
          <tr>
            <td><strong>8</strong></td>
            <td class="center"><span class="math-inline">$[0.95, 0.03, 0.02]$</span></td>
            <td class="center"><span class="math-inline">$[0.93, 0.04, 0.03]$</span></td>
            <td class="center"><span class="math-inline">$0.021$</span></td>
            <td class="center" style="color:#28a745;">Excellent match</td>
          </tr>
        </tbody>
      </table>

       <div class="math-display">
         <strong>Calculation for Sample 1 (hard):</strong><br><br>
         $$\mathcal{L}(y,\hat{y}) = -[1\cdot\log(0.8) + 0\cdot\log(0.1) + 0\cdot\log(0.1)] = -\log(0.8) = 0.223.$$
       </div>

       <div class="math-display">
         <strong>Calculation for Sample 6 (soft):</strong><br><br>
         $$\mathcal{L}(y,\hat{y}) = -[0.7\log(0.6) + 0.2\log(0.3) + 0.1\log(0.1)]$$
         $$= -[0.7(-0.511) + 0.2(-1.204) + 0.1(-2.303)] = 0.829.$$
       </div>

      <h3>Key Observations</h3>
      <ul>
        <li><strong>Perfect prediction:</strong> If $y=\hat{y}$, CE equals the entropy $H(y)$</li>
        <li><strong>Confident & wrong:</strong> CE grows quickly when the model is confidently wrong</li>
        <li><strong>Uncertain:</strong> Near-uniform predictions give moderate loss</li>
        <li><strong>Soft labels:</strong> Naturally handled by the same CE formula</li>
      </ul>
    </div>
  </div>
</main>

 <script>
   // Function to show code examples
   function showCodeExamples() {
     const modal = document.createElement('div');
     modal.style.cssText = `
       position: fixed; top: 0; left: 0; width: 100%; height: 100%;
       background: rgba(0,0,0,0.8); z-index: 10000; display: flex;
       justify-content: center; align-items: center; padding: 2rem;
     `;
     
     const content = document.createElement('div');
     content.style.cssText = `
       background: white; border-radius: 15px; max-width: 90%; max-height: 90%;
       overflow-y: auto; padding: 2rem; position: relative;
     `;
     
     content.innerHTML = `
       <button onclick="closeModal(this)" style="
         position: absolute; top: 1rem; right: 1rem; background: #dc3545;
         color: white; border: none; border-radius: 50%; width: 30px; height: 30px;
         cursor: pointer; font-size: 1.2rem;
       ">√ó</button>
       
       <h2 style="color: #667eea; margin-bottom: 2rem;">üíª Code Examples</h2>
       
       <div class="code-section">
         <h3>1. NumPy Implementation</h3>
         <p>Pure NumPy implementation of CrossEntropy loss for educational purposes:</p>
         
         <div class="code-container">
           <div class="code-header">
             <span>crossentropy_numpy.py</span>
             <button class="copy-btn" onclick="copyCode(this)">Copy</button>
           </div>
           <pre><code class="language-python">import numpy as np

def crossentropy_loss(y_true, y_pred, epsilon=1e-15):
    """
    Compute CrossEntropy loss between true and predicted distributions.
    
    Args:
        y_true: True labels (one-hot or soft) shape (N, C)
        y_pred: Predicted probabilities shape (N, C)
        epsilon: Small value to avoid log(0)
    
    Returns:
        CrossEntropy loss (scalar)
    """
    # Clip predictions to avoid log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    # Compute CrossEntropy: -sum(y_true * log(y_pred))
    loss = -np.sum(y_true * np.log(y_pred), axis=1)
    
    # Return mean loss across samples
    return np.mean(loss)

# Example usage
y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # One-hot
y_pred = np.array([[0.8, 0.1, 0.1], [0.3, 0.4, 0.3], [0.2, 0.3, 0.5]])

loss = crossentropy_loss(y_true, y_pred)
print(f"CrossEntropy Loss: {loss:.4f}")</code></pre>
         </div>
       </div>

       <div class="code-section">
         <h3>2. PyTorch Implementation</h3>
         <p>PyTorch implementation with automatic differentiation:</p>
         
         <div class="code-container">
           <div class="code-header">
             <span>crossentropy_pytorch.py</span>
             <button class="copy-btn" onclick="copyCode(this)">Copy</button>
           </div>
           <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class CrossEntropyLoss(nn.Module):
    """Custom CrossEntropy loss implementation"""
    
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction
    
    def forward(self, y_pred, y_true):
        """
        Args:
            y_pred: Logits or probabilities shape (N, C)
            y_true: True labels shape (N,) or (N, C)
        """
        if y_true.dim() == 1:
            # Convert class indices to one-hot
            y_true = F.one_hot(y_true, num_classes=y_pred.size(1)).float()
        
        # Compute CrossEntropy
        loss = -torch.sum(y_true * F.log_softmax(y_pred, dim=1), dim=1)
        
        if self.reduction == 'mean':
            return torch.mean(loss)
        elif self.reduction == 'sum':
            return torch.sum(loss)
        else:
            return loss

# Example usage
model = nn.Linear(10, 3)  # 10 features, 3 classes
criterion = CrossEntropyLoss()

# Sample data
x = torch.randn(32, 10)  # 32 samples, 10 features
y_true = torch.randint(0, 3, (32,))  # Class indices

# Forward pass
logits = model(x)
loss = criterion(logits, y_true)

print(f"Loss: {loss.item():.4f}")

# Backward pass
loss.backward()</code></pre>
         </div>
       </div>

       <div class="code-section">
         <h3>3. TensorFlow/Keras Implementation</h3>
         <p>TensorFlow implementation using built-in functions:</p>
         
         <div class="code-container">
           <div class="code-header">
             <span>crossentropy_tensorflow.py</span>
             <button class="copy-btn" onclick="copyCode(this)">Copy</button>
           </div>
           <pre><code class="language-python">import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import CategoricalCrossentropy

# Method 1: Using Keras built-in loss
def build_model():
    model = Sequential([
        Dense(64, activation='relu', input_shape=(10,)),
        Dense(32, activation='relu'),
        Dense(3, activation='softmax')  # 3 classes
    ])
    
    # Use built-in CrossEntropy loss
    model.compile(
        optimizer='adam',
        loss=CategoricalCrossentropy(),
        metrics=['accuracy']
    )
    return model

# Method 2: Custom implementation
def custom_crossentropy(y_true, y_pred):
    """Custom CrossEntropy loss function"""
    # Clip to avoid log(0)
    y_pred = tf.clip_by_value(y_pred, 1e-15, 1 - 1e-15)
    
    # Compute CrossEntropy
    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1)
    return tf.reduce_mean(loss)

# Example usage
model = build_model()

# Sample data
x_train = tf.random.normal((1000, 10))
y_train = tf.one_hot(tf.random.uniform((1000,), 0, 3, dtype=tf.int32), 3)

# Train model
model.fit(x_train, y_train, epochs=10, batch_size=32)

# Evaluate
x_test = tf.random.normal((100, 10))
y_test = tf.one_hot(tf.random.uniform((100,), 0, 3, dtype=tf.int32), 3)
loss, accuracy = model.evaluate(x_test, y_test)

print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")</code></pre>
         </div>
       </div>

       <div class="code-section">
         <h3>4. Gradient Computation</h3>
         <p>Manual computation of CrossEntropy gradients:</p>
         
         <div class="code-container">
           <div class="code-header">
             <span>crossentropy_gradients.py</span>
             <button class="copy-btn" onclick="copyCode(this)">Copy</button>
           </div>
           <pre><code class="language-python">import numpy as np

def softmax(x):
    """Compute softmax probabilities"""
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def crossentropy_gradient(y_true, logits):
    """
    Compute gradient of CrossEntropy loss w.r.t. logits.
    
    Key result: ‚àÇL/‚àÇz_i = y_pred_i - y_true_i
    """
    # Convert logits to probabilities
    y_pred = softmax(logits)
    
    # Gradient: y_pred - y_true
    gradient = y_pred - y_true
    
    return gradient

# Example usage
np.random.seed(42)

# Sample data
N, C = 5, 3  # 5 samples, 3 classes
logits = np.random.randn(N, C)
y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])

# Compute gradients
gradients = crossentropy_gradient(y_true, logits)

print("Logits:")
print(logits)
print("\nTrue labels:")
print(y_true)
print("\nPredicted probabilities:")
print(softmax(logits))
print("\nGradients (‚àÇL/‚àÇz):")
print(gradients)

# Verify: gradients should equal (y_pred - y_true)
y_pred = softmax(logits)
manual_gradients = y_pred - y_true
print("\nManual gradients (y_pred - y_true):")
print(manual_gradients)
print(f"\nGradients match: {np.allclose(gradients, manual_gradients)}")</code></pre>
         </div>
       </div>
     `;
     
     modal.appendChild(content);
     document.body.appendChild(modal);
     
     // Prevent body scroll when modal is open
     document.body.classList.add('modal-open');
     
     // Add click outside to close
     modal.addEventListener('click', function(e) {
       if (e.target === modal) {
         closeModal(modal);
       }
     });
     
     // Initialize Prism highlighting
     if (typeof Prism !== 'undefined') {
       Prism.highlightAll();
     }
   }

   // Function to properly close modal
   function closeModal(element) {
     const modal = element.closest('[style*="position: fixed"]') || element;
     if (modal && modal.parentNode) {
       modal.parentNode.removeChild(modal);
     }
     // Restore body scroll and remove modal class
     document.body.classList.remove('modal-open');
     document.body.style.overflow = '';
   }

   // Function to copy code to clipboard
   function copyCode(button) {
     const codeBlock = button.parentElement.nextElementSibling.querySelector('code');
     const text = codeBlock.textContent;
     
     navigator.clipboard.writeText(text).then(() => {
       const originalText = button.textContent;
       button.textContent = 'Copied!';
       button.classList.add('copied');
       
       setTimeout(() => {
         button.textContent = originalText;
         button.classList.remove('copied');
       }, 2000);
     }).catch(err => {
       console.error('Failed to copy: ', err);
       button.textContent = 'Failed';
     });
   }

   // Click handlers for the section cards
   document.querySelectorAll('.section-card').forEach(card => {
     card.addEventListener('click', function () {
       const title = this.querySelector('h4').textContent;
       let content = '';
       switch (title) {
         case 'üíª Code Examples':
           showCodeExamples();
           return;
         case 'üìä Visualizations':
           content = 'Visualizations will be implemented here...';
           break;
         case 'üèãÔ∏è Exercises':
           content = 'Exercises will be implemented here...';
           break;
       }
       alert(`${title}\n\n${content}`);
     });
   });

   // Smooth scrolling for internal links
   document.querySelectorAll('a[href^="#"]').forEach(anchor => {
     anchor.addEventListener('click', function (e) {
       e.preventDefault();
       const target = document.querySelector(this.getAttribute('href'));
       if (target) {
         target.scrollIntoView({ behavior: 'smooth', block: 'start' });
       }
     });
   });
 </script>
</body>
</html>