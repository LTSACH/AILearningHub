<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Softmax Regression - AI Learning Hub</title>

  <!-- MathJax Configuration -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Prism.js for syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

  <!-- Chart.js for visualizations -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6; color: #333;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
    }
    .container { max-width: 1200px; margin: 0 auto; padding: 0 20px; }
    header {
      background: rgba(255, 255, 255, 0.95); backdrop-filter: blur(10px);
      box-shadow: 0 2px 20px rgba(0,0,0,0.1); position: sticky; top: 0; z-index: 1000;
    }
    nav { display: flex; justify-content: space-between; align-items: center; padding: 1rem 0; }
    .logo { font-size: 1.5rem; font-weight: bold; color: #667eea; text-decoration: none; }
    .breadcrumb { font-size: 0.9rem; color: #666; }
    .breadcrumb a { color: #667eea; text-decoration: none; }
    .breadcrumb a:hover { text-decoration: underline; }
    main { padding: 2rem 0; }
    .content-card {
      background: rgba(255,255,255,0.95);
      border-radius: 15px; padding: 2rem; margin-bottom: 2rem;
      box-shadow: 0 8px 32px rgba(0,0,0,0.1); backdrop-filter: blur(10px);
    }
    h1 { color: #667eea; margin-bottom: 1rem; font-size: 2.5rem; }
    h2 {
      color: #667eea; margin: 2rem 0 1rem 0; font-size: 1.8rem;
      border-bottom: 2px solid #667eea; padding-bottom: 0.5rem;
    }
    h3 { color: #555; margin: 1.5rem 0 0.5rem 0; font-size: 1.3rem; }
    .overview {
      background: linear-gradient(135deg, #ff6b6b, #ee5a24);
      color: white; border-radius: 10px; padding: 1.5rem; margin: 1rem 0;
    }
    .overview h3 { color: white; margin-top: 0; }
    .math-display {
      margin: 1.5rem 0; text-align: center; font-size: 1.2rem;
    }
    .math-inline {
      font-size: 1rem;
    }
    .learning-objectives {
      background: linear-gradient(135deg, #4ecdc4, #44a08d);
      color: white; border-radius: 10px; padding: 1.5rem; margin: 1rem 0;
    }
    .learning-objectives h3 { color: white; margin-top: 0; }
    .learning-objectives ul { list-style: none; padding-left: 0; }
    .learning-objectives li { margin: 0.5rem 0; padding-left: 1.5rem; position: relative; }
    .learning-objectives li::before {
      content: "‚úì"; position: absolute; left: 0; color: #fff; font-weight: bold;
    }
    .estimated-time {
      background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px;
      padding: 1rem; margin: 1rem 0; color: #856404;
    }
    .estimated-time strong { color: #667eea; }
    .back-button {
      display: inline-block; background: linear-gradient(45deg, #667eea, #764ba2);
      color: white; padding: 0.8rem 1.5rem; text-decoration: none; border-radius: 25px;
      font-weight: bold; transition: transform 0.3s ease, box-shadow 0.3s ease;
      box-shadow: 0 4px 15px rgba(102,126,234,0.4); margin-bottom: 2rem;
    }
    .back-button:hover { transform: translateY(-2px); box-shadow: 0 6px 20px rgba(102,126,234,0.6); }
    table {
      width: 100%; border-collapse: collapse; margin: 1rem 0; background: white;
      border-radius: 8px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    thead tr { background: #667eea; color: white; }
    th, td { padding: 12px; }
    th { text-align: left; }
    td.center { text-align: center; }
    .chart-container {
      position: relative; height: 400px; margin: 2rem 0;
      background: white; border-radius: 10px; padding: 1rem;
      box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    .comparison-grid {
      display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 2rem; margin: 2rem 0;
    }
    .comparison-card {
      background: white; border-radius: 10px; padding: 1.5rem;
      box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    .comparison-card h4 {
      color: #667eea; margin-bottom: 1rem; font-size: 1.2rem;
    }
    .code-container {
      position: relative; margin: 1rem 0;
    }
    .code-header {
      background: #667eea; color: white; padding: 0.5rem 1rem;
      border-radius: 8px 8px 0 0; font-weight: bold;
      display: flex; justify-content: space-between; align-items: center;
    }
    .copy-btn {
      background: rgba(255, 255, 255, 0.2); border: 1px solid rgba(255, 255, 255, 0.3);
      color: white; padding: 0.25rem 0.75rem; border-radius: 4px;
      cursor: pointer; font-size: 0.8rem; transition: all 0.3s ease;
    }
    .copy-btn:hover { background: rgba(255, 255, 255, 0.3); }
    .copy-btn.copied { background: #28a745; border-color: #28a745; }
    pre[class*="language-"] {
      margin: 0; border-radius: 0 0 8px 8px;
      border: 1px solid #e9ecef; border-top: none;
    }
    .section-nav {
      display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 1rem; margin: 2rem 0;
    }
    .section-card {
      background: #f8f9fa; border-radius: 10px; padding: 1.5rem; text-align: center;
      transition: transform 0.3s ease, box-shadow 0.3s ease; cursor: pointer;
    }
    .section-card:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
    .section-card h4 { color: #667eea; margin-bottom: 0.5rem; }
    .section-card p { color: #666; font-size: 0.9rem; }
    .info-box {
      background: #d1ecf1; border: 1px solid #bee5eb; border-radius: 8px;
      padding: 1rem; margin: 1rem 0; color: #0c5460;
    }
    .info-box strong { color: #0c5460; }
    @media (max-width: 768px) {
      h1 { font-size: 2rem; }
      .content-card { padding: 1.5rem; }
      .comparison-grid { grid-template-columns: 1fr; }
      .section-nav { grid-template-columns: 1fr; }
    }
  </style>
</head>
<body>
<header>
  <nav class="container">
    <a href="/AILearningHub/" class="logo">ü§ñ AI Learning Hub</a>
    <div class="breadcrumb">
      <a href="/AILearningHub/">Home</a> /
      <a href="/AILearningHub/02_Machine_Learning/">Machine Learning</a> /
      Softmax Regression
    </div>
  </nav>
</header>

<main>
  <div class="container">
    <a href="/AILearningHub/02_Machine_Learning/" class="back-button">‚Üê Back to Machine Learning</a>

    <div class="content-card">
      <h1>Softmax Regression</h1>

      <div class="overview">
        <h3>üìã Overview</h3>
        <p>Softmax Regression (also called Multinomial Logistic Regression) extends logistic regression to multiclass classification problems. It uses the softmax function to convert raw scores (logits) into probability distributions over multiple classes.</p>
      </div>

      <div class="learning-objectives">
        <h3>üéØ Learning Objectives</h3>
        <ul>
          <li>Understand the mathematical foundation of softmax regression</li>
          <li>Derive the softmax function and its properties</li>
          <li>Implement softmax regression from MLE perspective</li>
          <li>Apply softmax regression to multiclass problems</li>
          <li>Compare with binary logistic regression</li>
        </ul>
      </div>

      <div class="estimated-time">
        <strong>‚è±Ô∏è Estimated Time:</strong> 30‚Äì35 minutes reading + 60 minutes practice
      </div>

      <h2>Mathematical Foundation</h2>
      
      <div class="info-box">
        <strong>‚ÑπÔ∏è Note:</strong> Softmax Regression is the natural extension of Logistic Regression to handle <strong>more than two classes</strong> (K > 2).
      </div>

      <h3>The Softmax Function</h3>
      <p>The softmax function converts a vector of K real numbers into a probability distribution over K classes:</p>

      <div class="math-display">
        <strong>Softmax Function:</strong><br><br>
        $$\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, 2, ..., K$$
      </div>

      <p>Key properties of the softmax function:</p>
      <ul>
        <li><strong>Normalization:</strong> Œ£(i=1 to K) œÉ(z_i) = 1</li>
        <li><strong>Range:</strong> œÉ(z_i) ‚àà (0, 1) for all i</li>
        <li><strong>Argmax Property:</strong> argmax(z) = argmax(œÉ(z))</li>
        <li><strong>Differentiable:</strong> Smooth everywhere</li>
      </ul>

      <h3>Softmax Regression Model</h3>
      <p>For multiclass classification with K classes, we model the probability of each class:</p>

      <div class="math-display">
        <strong>Model:</strong><br><br>
        $$P(y=k|\boldsymbol{x}) = \frac{e^{\boldsymbol{w}_k^T \boldsymbol{x} + b_k}}{\sum_{j=1}^{K} e^{\boldsymbol{w}_j^T \boldsymbol{x} + b_j}}$$
      </div>

      <p>Where:
        <ul>
          <li><span class="math-inline">$\boldsymbol{x}$</span> is the feature vector</li>
          <li><span class="math-inline">$\boldsymbol{w}_k$</span> is the weight vector for class k</li>
          <li><span class="math-inline">$b_k$</span> is the bias term for class k</li>
          <li><span class="math-inline">$P(y=k|\boldsymbol{x})$</span> is the probability of class k</li>
        </ul>
      </p>

      <h3>Matrix Form</h3>
      <p>We can write this more compactly using matrix notation:</p>

      <div class="math-display">
        <strong>Logits:</strong><br><br>
        $$\boldsymbol{z} = \boldsymbol{W}^T \boldsymbol{x} + \boldsymbol{b}$$
      </div>

      <div class="math-display">
        <strong>Probabilities:</strong><br><br>
        $$\boldsymbol{p} = \text{softmax}(\boldsymbol{z})$$
      </div>

      <p>Where:
        <ul>
          <li><span class="math-inline">$\boldsymbol{W} \in \mathbb{R}^{d \times K}$</span> is the weight matrix</li>
          <li><span class="math-inline">$\boldsymbol{b} \in \mathbb{R}^K$</span> is the bias vector</li>
          <li><span class="math-inline">$\boldsymbol{z} \in \mathbb{R}^K$</span> is the logits vector</li>
          <li><span class="math-inline">$\boldsymbol{p} \in \mathbb{R}^K$</span> is the probability vector</li>
        </ul>
      </p>

      <h2>Theoretical Foundation: Maximum Likelihood Estimation</h2>
      
      <h3>Assumption</h3>
      <p>We assume that <span class="math-inline">$y_i$</span> follows a categorical distribution:</p>

      <div class="math-display">
        $$y_i | \boldsymbol{x}_i \sim \text{Categorical}(\boldsymbol{p}_i)$$
      </div>

      <p>Where <span class="math-inline">$\boldsymbol{p}_i = \text{softmax}(\boldsymbol{W}^T \boldsymbol{x}_i + \boldsymbol{b})$</span>.</p>

      <h3>Likelihood Function</h3>
      <p>For one-hot encoded labels, the probability mass function is:</p>

      <div class="math-display">
        $$P(y_i = k | \boldsymbol{x}_i, \boldsymbol{W}, \boldsymbol{b}) = p_{i,k} = \frac{e^{\boldsymbol{w}_k^T \boldsymbol{x}_i + b_k}}{\sum_{j=1}^{K} e^{\boldsymbol{w}_j^T \boldsymbol{x}_i + b_j}}$$
      </div>

      <p>For all <span class="math-inline">$n$</span> observations, the likelihood function is:</p>

      <div class="math-display">
        $$L(\boldsymbol{W}, \boldsymbol{b}) = \prod_{i=1}^{n} \prod_{k=1}^{K} p_{i,k}^{y_{i,k}}$$
      </div>

      <p>Where <span class="math-inline">$y_{i,k}$</span> is 1 if sample i belongs to class k, 0 otherwise (one-hot encoding).</p>

      <h3>Log-Likelihood</h3>
      <p>Taking the natural logarithm:</p>

      <div class="math-display">
        $$\ell(\boldsymbol{W}, \boldsymbol{b}) = \sum_{i=1}^{n} \sum_{k=1}^{K} y_{i,k} \log(p_{i,k})$$
      </div>

      <h3>Cross-Entropy Loss</h3>
      <p>To minimize (instead of maximize), we use the negative log-likelihood:</p>

      <div class="math-display">
        $$J(\boldsymbol{W}, \boldsymbol{b}) = -\frac{1}{n}\sum_{i=1}^{n} \sum_{k=1}^{K} y_{i,k} \log(p_{i,k})$$
      </div>

      <p>This is exactly the <strong>categorical cross-entropy loss function</strong>!</p>

      <h3>Gradient Derivation</h3>
      <p>The gradient of the loss with respect to <span class="math-inline">$\boldsymbol{w}_k$</span> is:</p>

      <div class="math-display">
        $$\frac{\partial J}{\partial \boldsymbol{w}_k} = \frac{1}{n}\sum_{i=1}^{n} (p_{i,k} - y_{i,k})\boldsymbol{x}_i$$
      </div>

      <p>And with respect to <span class="math-inline">$b_k$</span>:</p>

      <div class="math-display">
        $$\frac{\partial J}{\partial b_k} = \frac{1}{n}\sum_{i=1}^{n} (p_{i,k} - y_{i,k})$$
      </div>

      <h2>Key Properties</h2>
      <div class="comparison-grid">
        <div class="comparison-card">
          <h4>üìä Probability Distribution</h4>
          <p>Outputs valid probability distributions over all classes.</p>
        </div>

        <div class="comparison-card">
          <h4>üéØ Multiclass Support</h4>
          <p>Handles any number of classes K ‚â• 2 naturally.</p>
        </div>

        <div class="comparison-card">
          <h4>üìà Smooth Function</h4>
          <p>Softmax function is smooth and differentiable everywhere.</p>
        </div>

        <div class="comparison-card">
          <h4>üîç Interpretable</h4>
          <p>Probabilities can be directly interpreted as confidence scores.</p>
        </div>

        <div class="comparison-card">
          <h4>‚ö° Fast Training</h4>
          <p>Convex optimization problem with unique global minimum.</p>
        </div>

        <div class="comparison-card">
          <h4>üö´ No Assumptions</h4>
          <p>No assumptions about feature distributions.</p>
        </div>
      </div>

      <h2>Applications</h2>
      <ul>
        <li><strong>Computer Vision:</strong> Image classification (CIFAR-10, ImageNet), object detection</li>
        <li><strong>NLP:</strong> Text classification, sentiment analysis, language detection, topic modeling</li>
        <li><strong>Healthcare:</strong> Disease classification, medical image analysis, drug discovery</li>
        <li><strong>Finance:</strong> Risk rating, customer segmentation, fraud detection</li>
        <li><strong>Engineering:</strong> Quality classification, fault diagnosis, system monitoring</li>
      </ul>

      <h2>Interactive Visualization</h2>
      <p>Explore the softmax function with different logit values:</p>
      
      <div class="chart-container">
        <canvas id="softmaxChart"></canvas>
      </div>

      <h2>Comparison: Binary vs Multiclass</h2>
      <table>
        <thead>
          <tr>
            <th>Aspect</th>
            <th class="center">Logistic Regression (Binary)</th>
            <th class="center">Softmax Regression (Multiclass)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Number of Classes</strong></td>
            <td class="center">2 (K = 2)</td>
            <td class="center">Multiple (K > 2)</td>
          </tr>
          <tr>
            <td><strong>Activation Function</strong></td>
            <td class="center">Sigmoid œÉ(z)</td>
            <td class="center">Softmax œÉ(z)</td>
          </tr>
          <tr>
            <td><strong>Output Range</strong></td>
            <td class="center">P(y=1) ‚àà (0, 1)</td>
            <td class="center">Œ£ P(y=k) = 1</td>
          </tr>
          <tr>
            <td><strong>Parameters</strong></td>
            <td class="center">w ‚àà ‚Ñù·µà, b ‚àà ‚Ñù</td>
            <td class="center">W ‚àà ‚Ñù·µàÀ£·¥∑, b ‚àà ‚Ñù·¥∑</td>
          </tr>
          <tr>
            <td><strong>Loss Function</strong></td>
            <td class="center">Binary Cross-Entropy</td>
            <td class="center">Categorical Cross-Entropy</td>
          </tr>
          <tr>
            <td><strong>Decision Rule</strong></td>
            <td class="center">P(y=1) > 0.5</td>
            <td class="center">argmax P(y=k)</td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="section-nav">
      <div class="section-card">
        <h4>üíª Code Examples</h4>
        <p>NumPy, scikit-learn, and PyTorch implementations</p>
      </div>
      <div class="section-card">
        <h4>üìä Advanced Topics</h4>
        <p>One-vs-Rest, One-vs-One strategies</p>
      </div>
      <div class="section-card">
        <h4>üèãÔ∏è Exercises</h4>
        <p>Hands-on practice problems</p>
      </div>
    </div>

    <div class="content-card">
      <h2>Detailed Example: Handwritten Digit Classification</h2>
      <p>Let's work through a practical example of classifying handwritten digits (0-9) using softmax regression.</p>

      <h3>Problem Setup</h3>
      <p>We have 10 classes (digits 0-9) and want to predict the probability of each digit given pixel features.</p>

      <div class="math-display">
        $$P(\text{digit}=k|\boldsymbol{x}) = \frac{e^{\boldsymbol{w}_k^T \boldsymbol{x} + b_k}}{\sum_{j=0}^{9} e^{\boldsymbol{w}_j^T \boldsymbol{x} + b_j}}$$
      </div>

      <h3>Sample Prediction</h3>
      <p>For an input image <span class="math-inline">$\boldsymbol{x}$</span>, we compute logits for all 10 classes:</p>

      <div class="math-display">
        $$\boldsymbol{z} = \begin{bmatrix} z_0 \\ z_1 \\ z_2 \\ \vdots \\ z_9 \end{bmatrix} = \boldsymbol{W}^T \boldsymbol{x} + \boldsymbol{b}$$
      </div>

      <p>Suppose we get logits: <span class="math-inline">$\boldsymbol{z} = [2.1, -0.5, 0.8, 1.2, -1.1, 3.0, 0.3, -0.2, 1.5, 0.1]^T$</span></p>

      <h3>Softmax Calculation</h3>
      <p>First, compute the exponential of each logit:</p>

      <div class="math-display">
        $$e^{\boldsymbol{z}} = \begin{bmatrix} e^{2.1} \\ e^{-0.5} \\ e^{0.8} \\ \vdots \\ e^{0.1} \end{bmatrix} = \begin{bmatrix} 8.17 \\ 0.61 \\ 2.23 \\ \vdots \\ 1.11 \end{bmatrix}$$
      </div>

      <p>Sum of exponentials: <span class="math-inline">$\sum_{k=0}^{9} e^{z_k} = 8.17 + 0.61 + 2.23 + ... + 1.11 = 28.45$</span></p>

      <p>Final probabilities:</p>

      <div class="math-display">
        $$\boldsymbol{p} = \begin{bmatrix} P(0) \\ P(1) \\ P(2) \\ \vdots \\ P(9) \end{bmatrix} = \begin{bmatrix} 0.287 \\ 0.021 \\ 0.078 \\ \vdots \\ 0.039 \end{bmatrix}$$
      </div>

      <h3>Prediction</h3>
      <p>The predicted class is the one with the highest probability:</p>

      <div class="math-display">
        $$\hat{y} = \arg\max_{k} P(\text{digit}=k|\boldsymbol{x}) = \arg\max_{k} p_k = 0$$
      </div>

      <p>So the model predicts this is digit "0" with 28.7% confidence.</p>

      <h3>Training Process</h3>
      <p>During training, we minimize the categorical cross-entropy loss:</p>

      <div class="math-display">
        $$J(\boldsymbol{W}, \boldsymbol{b}) = -\frac{1}{n}\sum_{i=1}^{n} \sum_{k=0}^{9} y_{i,k} \log(p_{i,k})$$
      </div>

      <p>Where <span class="math-inline">$y_{i,k}$</span> is 1 if sample i is digit k, 0 otherwise.</p>
    </div>
  </div>
</main>

<script>
  // Initialize softmax function visualization
  function initSoftmaxChart() {
    const ctx = document.getElementById('softmaxChart').getContext('2d');
    
    // Example: 3-class softmax with different logit values
    const scenarios = [
      { name: 'Balanced Logits', logits: [1, 1, 1] },
      { name: 'One Dominant', logits: [3, 1, 1] },
      { name: 'Two Close', logits: [2, 2.1, 1] },
      { name: 'All Low', logits: [-1, -1, -1] }
    ];
    
    const datasets = scenarios.map((scenario, index) => {
      const expValues = scenario.logits.map(z => Math.exp(z));
      const sumExp = expValues.reduce((a, b) => a + b, 0);
      const probabilities = expValues.map(exp => exp / sumExp);
      
      return {
        label: scenario.name,
        data: probabilities,
        backgroundColor: [
          `rgba(102, 126, 234, ${0.7 - index * 0.1})`,
          `rgba(255, 107, 107, ${0.7 - index * 0.1})`,
          `rgba(78, 205, 196, ${0.7 - index * 0.1})`
        ],
        borderColor: [
          '#667eea',
          '#ff6b6b',
          '#4ecdc4'
        ],
        borderWidth: 2
      };
    });
    
    new Chart(ctx, {
      type: 'bar',
      data: {
        labels: ['Class 0', 'Class 1', 'Class 2'],
        datasets: datasets
      },
      options: {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
          title: {
            display: true,
            text: 'Softmax Function: Probability Distributions for Different Logit Values',
            font: {
              size: 16,
              weight: 'bold'
            }
          },
          legend: {
            display: true,
            position: 'top'
          }
        },
        scales: {
          x: {
            title: {
              display: true,
              text: 'Classes'
            },
            grid: {
              color: 'rgba(0,0,0,0.1)'
            }
          },
          y: {
            title: {
              display: true,
              text: 'Probability'
            },
            min: 0,
            max: 1,
            grid: {
              color: 'rgba(0,0,0,0.1)'
            }
          }
        },
        interaction: {
          intersect: false,
          mode: 'index'
        }
      }
    });
  }

  // Function to show code examples
  function showCodeExamples() {
    const modal = document.createElement('div');
    modal.id = 'code-modal';
    modal.style.cssText = `
      position: fixed; top: 0; left: 0; width: 100%; height: 100%;
      background: rgba(0,0,0,0.8); z-index: 10000; display: flex;
      justify-content: center; align-items: center; padding: 2rem;
    `;
    
    const content = document.createElement('div');
    content.style.cssText = `
      background: white; border-radius: 15px; max-width: 90%; max-height: 90%;
      overflow-y: auto; padding: 2rem; position: relative;
    `;
    
    content.innerHTML = `
      <button onclick="closeModal(this)" style="
        position: absolute; top: 1rem; right: 1rem; background: #dc3545;
        color: white; border: none; border-radius: 50%; width: 30px; height: 30px;
        cursor: pointer; font-size: 1.2rem;
      ">√ó</button>
      
      <h2 style="color: #667eea; margin-bottom: 2rem;">üíª Code Examples</h2>
      
      <div class="code-section">
        <h3>1. NumPy Implementation (From Scratch)</h3>
        <p>Implementing softmax regression using only NumPy:</p>
        
        <div class="code-container">
          <div class="code-header">
            <span>softmax_regression_numpy.py</span>
            <button class="copy-btn" onclick="copyCode(this)">Copy</button>
          </div>
          <pre><code class="language-python">import numpy as np

class SoftmaxRegression:
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.n_classes = None
    
    def softmax(self, z):
        """Softmax activation function with numerical stability."""
        # Subtract max for numerical stability
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)
    
    def fit(self, X, y):
        """
        Fit softmax regression model using gradient descent.
        
        Args:
            X: Feature matrix (n_samples, n_features)
            y: Target vector (n_samples,) with class labels
        """
        n_samples, n_features = X.shape
        self.n_classes = len(np.unique(y))
        
        # Convert labels to one-hot encoding
        y_onehot = np.eye(self.n_classes)[y]
        
        # Initialize weights and bias
        self.weights = np.random.randn(n_features, self.n_classes) * 0.01
        self.bias = np.zeros(self.n_classes)
        
        # Gradient descent
        for i in range(self.max_iterations):
            # Forward pass
            logits = np.dot(X, self.weights) + self.bias
            probabilities = self.softmax(logits)
            
            # Compute loss (categorical cross-entropy)
            loss = -np.mean(np.sum(y_onehot * np.log(probabilities + 1e-15), axis=1))
            
            # Compute gradients
            dw = (1 / n_samples) * np.dot(X.T, (probabilities - y_onehot))
            db = (1 / n_samples) * np.sum(probabilities - y_onehot, axis=0)
            
            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # Print progress
            if i % 100 == 0:
                print(f"Iteration {i}, Loss: {loss:.4f}")
    
    def predict_proba(self, X):
        """Return probability predictions."""
        logits = np.dot(X, self.weights) + self.bias
        return self.softmax(logits)
    
    def predict(self, X):
        """Return class predictions."""
        probabilities = self.predict_proba(X)
        return np.argmax(probabilities, axis=1)
    
    def score(self, X, y):
        """Calculate accuracy score."""
        predictions = self.predict(X)
        return np.mean(predictions == y)

# Example usage
np.random.seed(42)
X = np.random.randn(1000, 4)  # 1000 samples, 4 features
y = np.random.randint(0, 3, 1000)  # 3 classes

# Fit model
model = SoftmaxRegression(learning_rate=0.1, max_iterations=1000)
model.fit(X, y)

# Make predictions
y_pred = model.predict(X)
y_proba = model.predict_proba(X)

print(f"Accuracy: {model.score(X, y):.4f}")
print(f"Weights shape: {model.weights.shape}")
print(f"Bias shape: {model.bias.shape}")</code></pre>
        </div>
      </div>

      <div class="code-section">
        <h3>2. Scikit-learn Implementation</h3>
        <p>Using the popular scikit-learn library:</p>
        
        <div class="code-container">
          <div class="code-header">
            <span>softmax_regression_sklearn.py</span>
            <button class="copy-btn" onclick="copyCode(this)">Copy</button>
          </div>
          <pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import make_classification
import numpy as np

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=4, n_classes=3, 
                          n_redundant=0, n_informative=4, 
                          n_clusters_per_class=1, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit model (LogisticRegression with multi_class='multinomial' uses softmax)
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', 
                          random_state=42, max_iter=1000)
model.fit(X_train, y_train)

# Make predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)
y_proba_test = model.predict_proba(X_test)

# Evaluate model
print("Training Results:")
print(f"Accuracy: {accuracy_score(y_train, y_pred_train):.4f}")

print("\nTest Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_test))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_test))

print(f"\nModel Parameters:")
print(f"Intercept: {model.intercept_}")
print(f"Coefficients shape: {model.coef_.shape}")

# Show probability predictions for first 5 test samples
print(f"\nProbability Predictions (first 5 samples):")
for i in range(5):
    print(f"Sample {i+1}: {y_proba_test[i]}")</code></pre>
        </div>
      </div>

      <div class="code-section">
        <h3>3. PyTorch Implementation</h3>
        <p>Using PyTorch for neural network-style softmax regression:</p>
        
        <div class="code-container">
          <div class="code-header">
            <span>softmax_regression_pytorch.py</span>
            <button class="copy-btn" onclick="copyCode(this)">Copy</button>
          </div>
          <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class SoftmaxRegressionModel(nn.Module):
    def __init__(self, input_size, num_classes):
        super().__init__()
        self.linear = nn.Linear(input_size, num_classes)
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        logits = self.linear(x)
        return self.softmax(logits)

# Generate data
np.random.seed(42)
X = torch.randn(1000, 4, dtype=torch.float32)
y = torch.randint(0, 3, (1000,), dtype=torch.long)  # 3 classes

# Create model
model = SoftmaxRegressionModel(input_size=4, num_classes=3)
criterion = nn.CrossEntropyLoss()  # Categorical Cross-Entropy Loss
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Training loop
epochs = 1000
for epoch in range(epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 200 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Print learned parameters
print(f"Learned weights shape: {model.linear.weight.data.shape}")
print(f"Learned bias shape: {model.linear.bias.data.shape}")

# Make predictions
with torch.no_grad():
    predictions = model(X)
    predicted_classes = torch.argmax(predictions, dim=1)
    accuracy = (predicted_classes == y).float().mean()
    print(f"Training Accuracy: {accuracy.item():.4f}")

# Show some probability predictions
print(f"\nProbability Predictions (first 5 samples):")
with torch.no_grad():
    sample_predictions = model(X[:5])
    for i, prob in enumerate(sample_predictions):
        print(f"Sample {i+1}: {prob.numpy()}")</code></pre>
        </div>
      </div>
    `;
    
    modal.appendChild(content);
    document.body.appendChild(modal);
    
    // Prevent body scroll when modal is open
    document.body.classList.add('modal-open');
    
    // Add click outside to close
    modal.addEventListener('click', function(e) {
      if (e.target === modal) {
        closeModal(modal);
      }
    });
    
    // Initialize Prism highlighting
    if (typeof Prism !== 'undefined') {
      Prism.highlightAll();
    }
  }

  // Function to properly close modal
  function closeModal(element) {
    let modal = document.getElementById('code-modal');
    
    if (!modal && element && element.closest) {
      modal = element.closest('#code-modal');
    }
    
    if (!modal) {
      const modals = document.querySelectorAll('div[style*="position: fixed"][style*="z-index: 10000"]');
      modal = modals[modals.length - 1];
    }
    
    if (modal) {
      modal.remove();
    }
    
    // Restore body scroll and remove modal class
    document.body.classList.remove('modal-open');
    document.body.style.overflow = '';
    
    // Force cleanup - remove any remaining modals
    const remainingModals = document.querySelectorAll('div[style*="position: fixed"][style*="z-index: 10000"]');
    remainingModals.forEach(m => m.remove());
  }

  // Function to copy code to clipboard
  function copyCode(button) {
    const codeBlock = button.parentElement.nextElementSibling.querySelector('code');
    const text = codeBlock.textContent;
    
    navigator.clipboard.writeText(text).then(() => {
      const originalText = button.textContent;
      button.textContent = 'Copied!';
      button.classList.add('copied');
      
      setTimeout(() => {
        button.textContent = originalText;
        button.classList.remove('copied');
      }, 2000);
    }).catch(err => {
      console.error('Failed to copy: ', err);
      button.textContent = 'Failed';
    });
  }

  // Click handlers for the section cards
  document.querySelectorAll('.section-card').forEach(card => {
    card.addEventListener('click', function () {
      const title = this.querySelector('h4').textContent;
      switch (title) {
        case 'üíª Code Examples':
          showCodeExamples();
          break;
        case 'üìä Advanced Topics':
          alert('Advanced Topics: One-vs-Rest and One-vs-One strategies will be implemented here...');
          break;
        case 'üèãÔ∏è Exercises':
          alert('Exercises: Hands-on practice problems will be implemented here...');
          break;
      }
    });
  });

  // Initialize when page loads
  document.addEventListener('DOMContentLoaded', function() {
    initSoftmaxChart();
  });

  // Smooth scrolling for internal links
  document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
      e.preventDefault();
      const target = document.querySelector(this.getAttribute('href'));
      if (target) {
        target.scrollIntoView({ behavior: 'smooth', block: 'start' });
      }
    });
  });
</script>
</body>
</html>
