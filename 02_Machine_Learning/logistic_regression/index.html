<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Logistic Regression - AI Learning Hub</title>

  <!-- MathJax Configuration -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Prism.js for syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

  <!-- Chart.js for visualizations -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6; color: #333;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
    }
    .container { max-width: 1200px; margin: 0 auto; padding: 0 20px; }
    header {
      background: rgba(255, 255, 255, 0.95); backdrop-filter: blur(10px);
      box-shadow: 0 2px 20px rgba(0,0,0,0.1); position: sticky; top: 0; z-index: 1000;
    }
    nav { display: flex; justify-content: space-between; align-items: center; padding: 1rem 0; }
    .logo { font-size: 1.5rem; font-weight: bold; color: #667eea; text-decoration: none; }
    .breadcrumb { font-size: 0.9rem; color: #666; }
    .breadcrumb a { color: #667eea; text-decoration: none; }
    .breadcrumb a:hover { text-decoration: underline; }
    main { padding: 2rem 0; }
    .content-card {
      background: rgba(255,255,255,0.95);
      border-radius: 15px; padding: 2rem; margin-bottom: 2rem;
      box-shadow: 0 8px 32px rgba(0,0,0,0.1); backdrop-filter: blur(10px);
    }
    h1 { color: #667eea; margin-bottom: 1rem; font-size: 2.5rem; }
    h2 {
      color: #667eea; margin: 2rem 0 1rem 0; font-size: 1.8rem;
      border-bottom: 2px solid #667eea; padding-bottom: 0.5rem;
    }
    h3 { color: #555; margin: 1.5rem 0 0.5rem 0; font-size: 1.3rem; }
    .overview {
      background: linear-gradient(135deg, #ff6b6b, #ee5a24);
      color: white; border-radius: 10px; padding: 1.5rem; margin: 1rem 0;
    }
    .overview h3 { color: white; margin-top: 0; }
    .math-display {
      margin: 1.5rem 0; text-align: center; font-size: 1.2rem;
    }
    .math-inline {
      font-size: 1rem;
    }
    .learning-objectives {
      background: linear-gradient(135deg, #4ecdc4, #44a08d);
      color: white; border-radius: 10px; padding: 1.5rem; margin: 1rem 0;
    }
    .learning-objectives h3 { color: white; margin-top: 0; }
    .learning-objectives ul { list-style: none; padding-left: 0; }
    .learning-objectives li { margin: 0.5rem 0; padding-left: 1.5rem; position: relative; }
    .learning-objectives li::before {
      content: "‚úì"; position: absolute; left: 0; color: #fff; font-weight: bold;
    }
    .estimated-time {
      background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px;
      padding: 1rem; margin: 1rem 0; color: #856404;
    }
    .estimated-time strong { color: #667eea; }
    .back-button {
      display: inline-block; background: linear-gradient(45deg, #667eea, #764ba2);
      color: white; padding: 0.8rem 1.5rem; text-decoration: none; border-radius: 25px;
      font-weight: bold; transition: transform 0.3s ease, box-shadow 0.3s ease;
      box-shadow: 0 4px 15px rgba(102,126,234,0.4); margin-bottom: 2rem;
    }
    .back-button:hover { transform: translateY(-2px); box-shadow: 0 6px 20px rgba(102,126,234,0.6); }
    table {
      width: 100%; border-collapse: collapse; margin: 1rem 0; background: white;
      border-radius: 8px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    thead tr { background: #667eea; color: white; }
    th, td { padding: 12px; }
    th { text-align: left; }
    td.center { text-align: center; }
    .chart-container {
      position: relative; height: 400px; margin: 2rem 0;
      background: white; border-radius: 10px; padding: 1rem;
      box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    .comparison-grid {
      display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 2rem; margin: 2rem 0;
    }
    .comparison-card {
      background: white; border-radius: 10px; padding: 1.5rem;
      box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    .comparison-card h4 {
      color: #667eea; margin-bottom: 1rem; font-size: 1.2rem;
    }
    .code-container {
      position: relative; margin: 1rem 0;
    }
    .code-header {
      background: #667eea; color: white; padding: 0.5rem 1rem;
      border-radius: 8px 8px 0 0; font-weight: bold;
      display: flex; justify-content: space-between; align-items: center;
    }
    .copy-btn {
      background: rgba(255, 255, 255, 0.2); border: 1px solid rgba(255, 255, 255, 0.3);
      color: white; padding: 0.25rem 0.75rem; border-radius: 4px;
      cursor: pointer; font-size: 0.8rem; transition: all 0.3s ease;
    }
    .copy-btn:hover { background: rgba(255, 255, 255, 0.3); }
    .copy-btn.copied { background: #28a745; border-color: #28a745; }
    pre[class*="language-"] {
      margin: 0; border-radius: 0 0 8px 8px;
      border: 1px solid #e9ecef; border-top: none;
    }
    .section-nav {
      display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 1rem; margin: 2rem 0;
    }
    .section-card {
      background: #f8f9fa; border-radius: 10px; padding: 1.5rem; text-align: center;
      transition: transform 0.3s ease, box-shadow 0.3s ease; cursor: pointer;
    }
    .section-card:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
    .section-card h4 { color: #667eea; margin-bottom: 0.5rem; }
    .section-card p { color: #666; font-size: 0.9rem; }
    .warning-box {
      background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px;
      padding: 1rem; margin: 1rem 0; color: #856404;
    }
    .warning-box strong { color: #dc3545; }
    @media (max-width: 768px) {
      h1 { font-size: 2rem; }
      .content-card { padding: 1.5rem; }
      .comparison-grid { grid-template-columns: 1fr; }
      .section-nav { grid-template-columns: 1fr; }
    }
  </style>
</head>
<body>
<header>
  <nav class="container">
    <a href="/AILearningHub/" class="logo">ü§ñ AI Learning Hub</a>
    <div class="breadcrumb">
      <a href="/AILearningHub/">Home</a> /
      <a href="/AILearningHub/02_Machine_Learning/">Machine Learning</a> /
      Logistic Regression
    </div>
  </nav>
</header>

<main>
  <div class="container">
    <a href="/AILearningHub/02_Machine_Learning/" class="back-button">‚Üê Back to Machine Learning</a>

    <div class="content-card">
      <h1>Logistic Regression</h1>

      <div class="overview">
        <h3>üìã Overview</h3>
        <p>Logistic Regression is a fundamental classification algorithm that models the probability of binary outcomes. Despite its name containing "regression," it's actually a classification algorithm that uses the logistic (sigmoid) function to map linear combinations of features to probabilities.</p>
      </div>

      <div class="learning-objectives">
        <h3>üéØ Learning Objectives</h3>
        <ul>
          <li>Understand the mathematical foundation of logistic regression</li>
          <li>Derive the sigmoid function and decision boundary</li>
          <li>Implement logistic regression from MLE perspective</li>
          <li>Apply logistic regression to binary classification problems</li>
          <li>Compare with linear regression for classification tasks</li>
        </ul>
      </div>

      <div class="estimated-time">
        <strong>‚è±Ô∏è Estimated Time:</strong> 25‚Äì30 minutes reading + 50 minutes practice
      </div>

      <h2>Mathematical Foundation</h2>
      
      <div class="warning-box">
        <strong>‚ö†Ô∏è Important:</strong> Despite being called "regression," Logistic Regression is actually a <strong>classification algorithm</strong> for binary problems (0 or 1).
      </div>

      <h3>The Sigmoid Function</h3>
      <p>The core of logistic regression is the sigmoid (logistic) function, which maps any real number to a probability between 0 and 1:</p>

      <div class="math-display">
        <strong>Sigmoid Function:</strong><br><br>
        $$\sigma(z) = \frac{1}{1 + e^{-z}}$$
      </div>

      <p>Key properties of the sigmoid function:</p>
      <ul>
        <li><strong>Range:</strong> œÉ(z) ‚àà (0, 1) for all real z</li>
        <li><strong>Symmetry:</strong> œÉ(-z) = 1 - œÉ(z)</li>
        <li><strong>Derivative:</strong> œÉ'(z) = œÉ(z)(1 - œÉ(z))</li>
        <li><strong>Asymptotes:</strong> lim(z‚Üí‚àû) œÉ(z) = 1, lim(z‚Üí-‚àû) œÉ(z) = 0</li>
      </ul>

      <h3>Logistic Regression Model</h3>
      <p>For binary classification, we model the probability of class 1:</p>

      <div class="math-display">
        <strong>Model:</strong><br><br>
        $$P(y=1|\boldsymbol{x}) = \sigma(\boldsymbol{x}^T \boldsymbol{w} + w_0) = \frac{1}{1 + e^{-(\boldsymbol{x}^T \boldsymbol{w} + w_0)}}$$
      </div>

      <p>Where:
        <ul>
          <li><span class="math-inline">$\boldsymbol{x}$</span> is the feature vector</li>
          <li><span class="math-inline">$\boldsymbol{w}$</span> is the weight vector</li>
          <li><span class="math-inline">$w_0$</span> is the bias term</li>
          <li><span class="math-inline">$P(y=1|\boldsymbol{x})$</span> is the probability of class 1</li>
        </ul>
      </p>

      <h3>Decision Boundary</h3>
      <p>The decision boundary occurs when the probability equals 0.5:</p>

      <div class="math-display">
        $$P(y=1|\boldsymbol{x}) = 0.5 \Rightarrow \boldsymbol{x}^T \boldsymbol{w} + w_0 = 0$$
      </div>

      <p>This defines a hyperplane that separates the two classes.</p>

      <h2>Theoretical Foundation: Maximum Likelihood Estimation</h2>
      
      <h3>Assumption</h3>
      <p>We assume that <span class="math-inline">$y_i$</span> follows a Bernoulli distribution:</p>

      <div class="math-display">
        $$y_i | \boldsymbol{x}_i \sim \text{Bernoulli}(p_i)$$
      </div>

      <p>Where <span class="math-inline">$p_i = \sigma(\boldsymbol{x}_i^T \boldsymbol{w} + w_0)$</span>.</p>

      <h3>Likelihood Function</h3>
      <p>The probability mass function for a Bernoulli random variable is:</p>

      <div class="math-display">
        $$P(y_i | \boldsymbol{x}_i, \boldsymbol{w}) = p_i^{y_i}(1-p_i)^{1-y_i}$$
      </div>

      <p>For all <span class="math-inline">$n$</span> observations, the likelihood function is:</p>

      <div class="math-display">
        $$L(\boldsymbol{w}) = \prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}$$
      </div>

      <h3>Log-Likelihood</h3>
      <p>Taking the natural logarithm:</p>

      <div class="math-display">
        $$\ell(\boldsymbol{w}) = \sum_{i=1}^{n} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]$$
      </div>

      <h3>Cross-Entropy Loss</h3>
      <p>To minimize (instead of maximize), we use the negative log-likelihood:</p>

      <div class="math-display">
        $$J(\boldsymbol{w}) = -\frac{1}{n}\sum_{i=1}^{n} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]$$
      </div>

      <p>This is exactly the <strong>binary cross-entropy loss function</strong>!</p>

      <h3>Gradient Derivation</h3>
      <p>The gradient of the loss with respect to <span class="math-inline">$\boldsymbol{w}$</span> is:</p>

      <div class="math-display">
        $$\frac{\partial J}{\partial \boldsymbol{w}} = \frac{1}{n}\sum_{i=1}^{n} (p_i - y_i)\boldsymbol{x}_i$$
      </div>

      <p><strong>Derivation:</strong></p>
      <div class="math-display">
        $$\frac{\partial J}{\partial \boldsymbol{w}} = -\frac{1}{n}\sum_{i=1}^{n} \left[\frac{y_i}{p_i} \frac{\partial p_i}{\partial \boldsymbol{w}} + \frac{1-y_i}{1-p_i} \frac{\partial (1-p_i)}{\partial \boldsymbol{w}}\right]$$
      </div>

      <div class="math-display">
        $$= -\frac{1}{n}\sum_{i=1}^{n} \left[\frac{y_i}{p_i} - \frac{1-y_i}{1-p_i}\right] \frac{\partial p_i}{\partial \boldsymbol{w}}$$
      </div>

      <div class="math-display">
        $$= -\frac{1}{n}\sum_{i=1}^{n} \frac{y_i - p_i}{p_i(1-p_i)} \cdot p_i(1-p_i) \boldsymbol{x}_i = \frac{1}{n}\sum_{i=1}^{n} (p_i - y_i)\boldsymbol{x}_i$$
      </div>

      <h2>Key Properties</h2>
      <div class="comparison-grid">
        <div class="comparison-card">
          <h4>üìä Probabilistic Output</h4>
          <p>Provides probability estimates, not just class predictions.</p>
        </div>

        <div class="comparison-card">
          <h4>üéØ Linear Decision Boundary</h4>
          <p>Creates linear separations in feature space.</p>
        </div>

        <div class="comparison-card">
          <h4>üìà Smooth Function</h4>
          <p>Sigmoid function is smooth and differentiable everywhere.</p>
        </div>

        <div class="comparison-card">
          <h4>üîç Interpretable</h4>
          <p>Coefficients can be interpreted as log-odds ratios.</p>
        </div>

        <div class="comparison-card">
          <h4>‚ö° Fast Training</h4>
          <p>Convex optimization problem with unique global minimum.</p>
        </div>

        <div class="comparison-card">
          <h4>üö´ No Assumptions</h4>
          <p>No assumptions about feature distributions (unlike LDA).</p>
        </div>
      </div>

      <h2>Applications</h2>
      <ul>
        <li><strong>Healthcare:</strong> Disease diagnosis, treatment outcome prediction, medical image analysis</li>
        <li><strong>Finance:</strong> Credit scoring, fraud detection, loan approval</li>
        <li><strong>Marketing:</strong> Customer churn prediction, conversion analysis, A/B testing</li>
        <li><strong>Engineering:</strong> Quality control, failure prediction, defect detection</li>
        <li><strong>Social Sciences:</strong> Survey analysis, behavioral prediction, policy evaluation</li>
      </ul>

      <h2>Interactive Visualization</h2>
      <p>Explore the sigmoid function and decision boundary:</p>
      
      <div class="chart-container">
        <canvas id="sigmoidChart"></canvas>
      </div>

      <h2>Comparison with Linear Regression</h2>
      <table>
        <thead>
          <tr>
            <th>Aspect</th>
            <th class="center">Linear Regression</th>
            <th class="center">Logistic Regression</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Problem Type</strong></td>
            <td class="center">Regression (continuous output)</td>
            <td class="center">Classification (binary output)</td>
          </tr>
          <tr>
            <td><strong>Output Range</strong></td>
            <td class="center">(-‚àû, +‚àû)</td>
            <td class="center">(0, 1)</td>
          </tr>
          <tr>
            <td><strong>Activation Function</strong></td>
            <td class="center">Linear (identity)</td>
            <td class="center">Sigmoid</td>
          </tr>
          <tr>
            <td><strong>Loss Function</strong></td>
            <td class="center">MSE</td>
            <td class="center">Cross-Entropy</td>
          </tr>
          <tr>
            <td><strong>Optimization</strong></td>
            <td class="center">Closed-form solution available</td>
            <td class="center">Iterative optimization required</td>
          </tr>
          <tr>
            <td><strong>Assumptions</strong></td>
            <td class="center">Gaussian errors</td>
            <td class="center">Bernoulli distribution</td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="section-nav">
      <div class="section-card">
        <h4>üíª Code Examples</h4>
        <p>NumPy, scikit-learn, and PyTorch implementations</p>
      </div>
      <div class="section-card">
        <h4>üìä Softmax Regression</h4>
        <p>Extension to multiclass classification</p>
      </div>
      <div class="section-card">
        <h4>üèãÔ∏è Exercises</h4>
        <p>Hands-on practice problems</p>
      </div>
    </div>

    <div class="content-card">
      <h2>Detailed Example: Email Spam Detection</h2>
      <p>Let's work through a practical example of detecting spam emails based on word frequencies.</p>

      <h3>Sample Data</h3>
      <table>
        <thead>
          <tr>
            <th>Email</th>
            <th class="center">"Free" Count</th>
            <th class="center">"Money" Count</th>
            <th class="center">Is Spam?</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>1</strong></td>
            <td class="center">3</td>
            <td class="center">1</td>
            <td class="center">1 (Spam)</td>
          </tr>
          <tr>
            <td><strong>2</strong></td>
            <td class="center">1</td>
            <td class="center">0</td>
            <td class="center">0 (Not Spam)</td>
          </tr>
          <tr>
            <td><strong>3</strong></td>
            <td class="center">5</td>
            <td class="center">2</td>
            <td class="center">1 (Spam)</td>
          </tr>
          <tr>
            <td><strong>4</strong></td>
            <td class="center">0</td>
            <td class="center">0</td>
            <td class="center">0 (Not Spam)</td>
          </tr>
          <tr>
            <td><strong>5</strong></td>
            <td class="center">2</td>
            <td class="center">1</td>
            <td class="center">1 (Spam)</td>
          </tr>
        </tbody>
      </table>

      <h3>Model Setup</h3>
      <p>We want to predict spam probability based on word counts:</p>

      <div class="math-display">
        $$P(\text{Spam}|\boldsymbol{x}) = \sigma(w_0 + w_1 \times \text{"Free"} + w_2 \times \text{"Money"})$$
      </div>

      <h3>Training Process</h3>
      <p>Using gradient descent to minimize cross-entropy loss:</p>

      <div class="math-display">
        $$J(\boldsymbol{w}) = -\frac{1}{n}\sum_{i=1}^{n} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]$$
      </div>

      <h3>Interpretation</h3>
      <p>After training, we might get:</p>

      <div class="math-display">
        $$\boldsymbol{w} = \begin{bmatrix} w_0 \\ w_1 \\ w_2 \end{bmatrix} = \begin{bmatrix} -2.5 \\ 1.2 \\ 0.8 \end{bmatrix}$$
      </div>

      <p>This means:</p>
      <ul>
        <li><strong>Bias $w_0$ (-2.5):</strong> Base log-odds when no keywords present</li>
        <li><strong>"Free" coefficient $w_1$ (1.2):</strong> Each "free" word increases log-odds by 1.2</li>
        <li><strong>"Money" coefficient $w_2$ (0.8):</strong> Each "money" word increases log-odds by 0.8</li>
      </ul>

      <h3>Prediction Example</h3>
      <p>For an email with 2 "free" words and 1 "money" word:</p>

      <div class="math-display">
        $$z = -2.5 + 1.2 \times 2 + 0.8 \times 1 = 0.7$$
      </div>

      <div class="math-display">
        $$P(\text{Spam}) = \sigma(0.7) = \frac{1}{1 + e^{-0.7}} = 0.67$$
      </div>

      <p>So there's a 67% chance this email is spam.</p>
    </div>
  </div>
</main>

<script>
  // Initialize sigmoid function visualization
  function initSigmoidChart() {
    const ctx = document.getElementById('sigmoidChart').getContext('2d');
    
    // Generate sigmoid function data
    const zValues = [];
    const sigmoidValues = [];
    const decisionBoundary = [];
    
    for (let i = -6; i <= 6; i += 0.1) {
      zValues.push(i);
      sigmoidValues.push(1 / (1 + Math.exp(-i)));
      decisionBoundary.push(0.5);
    }
    
    new Chart(ctx, {
      type: 'line',
      data: {
        labels: zValues.map(z => z.toFixed(1)),
        datasets: [{
          label: 'Sigmoid Function œÉ(z)',
          data: sigmoidValues,
          borderColor: '#667eea',
          backgroundColor: 'rgba(102, 126, 234, 0.1)',
          borderWidth: 3,
          fill: true,
          tension: 0.1
        }, {
          label: 'Decision Boundary (0.5)',
          data: decisionBoundary,
          borderColor: '#ff6b6b',
          borderWidth: 2,
          borderDash: [5, 5],
          pointRadius: 0
        }]
      },
      options: {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
          title: {
            display: true,
            text: 'Sigmoid Function: œÉ(z) = 1/(1 + e^(-z))',
            font: {
              size: 16,
              weight: 'bold'
            }
          },
          legend: {
            display: true,
            position: 'top'
          }
        },
        scales: {
          x: {
            title: {
              display: true,
              text: 'z = x^T w + w‚ÇÄ'
            },
            grid: {
              color: 'rgba(0,0,0,0.1)'
            }
          },
          y: {
            title: {
              display: true,
              text: 'Probability P(y=1|x)'
            },
            min: 0,
            max: 1,
            grid: {
              color: 'rgba(0,0,0,0.1)'
            }
          }
        },
        interaction: {
          intersect: false,
          mode: 'index'
        }
      }
    });
  }

  // Function to show code examples
  function showCodeExamples() {
    const modal = document.createElement('div');
    modal.id = 'code-modal';
    modal.style.cssText = `
      position: fixed; top: 0; left: 0; width: 100%; height: 100%;
      background: rgba(0,0,0,0.8); z-index: 10000; display: flex;
      justify-content: center; align-items: center; padding: 2rem;
    `;
    
    const content = document.createElement('div');
    content.style.cssText = `
      background: white; border-radius: 15px; max-width: 90%; max-height: 90%;
      overflow-y: auto; padding: 2rem; position: relative;
    `;
    
    content.innerHTML = `
      <button onclick="closeModal(this)" style="
        position: absolute; top: 1rem; right: 1rem; background: #dc3545;
        color: white; border: none; border-radius: 50%; width: 30px; height: 30px;
        cursor: pointer; font-size: 1.2rem;
      ">√ó</button>
      
      <h2 style="color: #667eea; margin-bottom: 2rem;">üíª Code Examples</h2>
      
      <div class="code-section">
        <h3>1. NumPy Implementation (From Scratch)</h3>
        <p>Implementing logistic regression using only NumPy:</p>
        
        <div class="code-container">
          <div class="code-header">
            <span>logistic_regression_numpy.py</span>
            <button class="copy-btn" onclick="copyCode(this)">Copy</button>
          </div>
          <pre><code class="language-python">import numpy as np

class LogisticRegression:
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
    
    def sigmoid(self, z):
        """Sigmoid activation function."""
        # Clip z to prevent overflow
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        """
        Fit logistic regression model using gradient descent.
        
        Args:
            X: Feature matrix (n_samples, n_features)
            y: Target vector (n_samples,)
        """
        n_samples, n_features = X.shape
        
        # Initialize weights and bias
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # Gradient descent
        for i in range(self.max_iterations):
            # Forward pass
            linear_model = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_model)
            
            # Compute loss (cross-entropy)
            loss = -np.mean(y * np.log(predictions + 1e-15) + 
                           (1 - y) * np.log(1 - predictions + 1e-15))
            
            # Compute gradients
            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))
            db = (1 / n_samples) * np.sum(predictions - y)
            
            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # Print progress
            if i % 100 == 0:
                print(f"Iteration {i}, Loss: {loss:.4f}")
    
    def predict_proba(self, X):
        """Return probability predictions."""
        linear_model = np.dot(X, self.weights) + self.bias
        return self.sigmoid(linear_model)
    
    def predict(self, X, threshold=0.5):
        """Return binary predictions."""
        probabilities = self.predict_proba(X)
        return (probabilities >= threshold).astype(int)
    
    def score(self, X, y):
        """Calculate accuracy score."""
        predictions = self.predict(X)
        return np.mean(predictions == y)

# Example usage
np.random.seed(42)
X = np.random.randn(100, 2)  # 100 samples, 2 features
y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Simple binary classification

# Fit model
model = LogisticRegression(learning_rate=0.1, max_iterations=1000)
model.fit(X, y)

# Make predictions
y_pred = model.predict(X)
y_proba = model.predict_proba(X)

print(f"Accuracy: {model.score(X, y):.4f}")
print(f"Weights: {model.weights}")
print(f"Bias: {model.bias:.4f}")</code></pre>
        </div>
      </div>

      <div class="code-section">
        <h3>2. Scikit-learn Implementation</h3>
        <p>Using the popular scikit-learn library:</p>
        
        <div class="code-container">
          <div class="code-header">
            <span>logistic_regression_sklearn.py</span>
            <button class="copy-btn" onclick="copyCode(this)">Copy</button>
          </div>
          <pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import make_classification
import numpy as np

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=4, n_redundant=0, 
                          n_informative=4, n_clusters_per_class=1, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit model
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train, y_train)

# Make predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)
y_proba_test = model.predict_proba(X_test)

# Evaluate model
print("Training Results:")
print(f"Accuracy: {accuracy_score(y_train, y_pred_train):.4f}")

print("\nTest Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_test))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_test))

print(f"\nModel Parameters:")
print(f"Intercept: {model.intercept_[0]:.4f}")
print(f"Coefficients: {model.coef_[0]}")

# Show probability predictions for first 5 test samples
print(f"\nProbability Predictions (first 5 samples):")
for i in range(5):
    print(f"Sample {i+1}: P(class=1) = {y_proba_test[i, 1]:.4f}")</code></pre>
        </div>
      </div>

      <div class="code-section">
        <h3>3. PyTorch Implementation</h3>
        <p>Using PyTorch for neural network-style logistic regression:</p>
        
        <div class="code-container">
          <div class="code-header">
            <span>logistic_regression_pytorch.py</span>
            <button class="copy-btn" onclick="copyCode(this)">Copy</button>
          </div>
          <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class LogisticRegressionModel(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.linear = nn.Linear(input_size, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        linear_output = self.linear(x)
        return self.sigmoid(linear_output)

# Generate data
np.random.seed(42)
X = torch.randn(1000, 3, dtype=torch.float32)
y = (X[:, 0] + X[:, 1] - X[:, 2] > 0).float().unsqueeze(1)  # Binary classification

# Create model
model = LogisticRegressionModel(input_size=3)
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Training loop
epochs = 1000
for epoch in range(epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 200 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Print learned parameters
print(f"Learned weights: {model.linear.weight.data}")
print(f"Learned bias: {model.linear.bias.data}")

# Make predictions
with torch.no_grad():
    predictions = model(X)
    predicted_classes = (predictions > 0.5).float()
    accuracy = (predicted_classes == y).float().mean()
    print(f"Training Accuracy: {accuracy.item():.4f}")

# Show some probability predictions
print(f"\nProbability Predictions (first 5 samples):")
with torch.no_grad():
    sample_predictions = model(X[:5])
    for i, prob in enumerate(sample_predictions):
        print(f"Sample {i+1}: P(class=1) = {prob.item():.4f}")</code></pre>
        </div>
      </div>
    `;
    
    modal.appendChild(content);
    document.body.appendChild(modal);
    
    // Prevent body scroll when modal is open
    document.body.classList.add('modal-open');
    
    // Add click outside to close
    modal.addEventListener('click', function(e) {
      if (e.target === modal) {
        closeModal(modal);
      }
    });
    
    // Initialize Prism highlighting
    if (typeof Prism !== 'undefined') {
      Prism.highlightAll();
    }
  }

  // Function to properly close modal
  function closeModal(element) {
    let modal = document.getElementById('code-modal');
    
    if (!modal && element && element.closest) {
      modal = element.closest('#code-modal');
    }
    
    if (!modal) {
      const modals = document.querySelectorAll('div[style*="position: fixed"][style*="z-index: 10000"]');
      modal = modals[modals.length - 1];
    }
    
    if (modal) {
      modal.remove();
    }
    
    // Restore body scroll and remove modal class
    document.body.classList.remove('modal-open');
    document.body.style.overflow = '';
    
    // Force cleanup - remove any remaining modals
    const remainingModals = document.querySelectorAll('div[style*="position: fixed"][style*="z-index: 10000"]');
    remainingModals.forEach(m => m.remove());
  }

  // Function to copy code to clipboard
  function copyCode(button) {
    const codeBlock = button.parentElement.nextElementSibling.querySelector('code');
    const text = codeBlock.textContent;
    
    navigator.clipboard.writeText(text).then(() => {
      const originalText = button.textContent;
      button.textContent = 'Copied!';
      button.classList.add('copied');
      
      setTimeout(() => {
        button.textContent = originalText;
        button.classList.remove('copied');
      }, 2000);
    }).catch(err => {
      console.error('Failed to copy: ', err);
      button.textContent = 'Failed';
    });
  }

  // Click handlers for the section cards
  document.querySelectorAll('.section-card').forEach(card => {
    card.addEventListener('click', function () {
      const title = this.querySelector('h4').textContent;
      switch (title) {
        case 'üíª Code Examples':
          showCodeExamples();
          break;
        case 'üìä Softmax Regression':
          window.location.href = '../softmax_regression/';
          break;
        case 'üèãÔ∏è Exercises':
          alert('Exercises: Hands-on practice problems will be implemented here...');
          break;
      }
    });
  });

  // Initialize when page loads
  document.addEventListener('DOMContentLoaded', function() {
    initSigmoidChart();
  });

  // Smooth scrolling for internal links
  document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
      e.preventDefault();
      const target = document.querySelector(this.getAttribute('href'));
      if (target) {
        target.scrollIntoView({ behavior: 'smooth', block: 'start' });
      }
    });
  });
</script>
</body>
</html>
