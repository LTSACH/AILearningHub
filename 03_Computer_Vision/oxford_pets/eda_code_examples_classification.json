{
  "dataset_overview": {
    "title": "Oxford Pets Dataset Overview",
    "explanation": {
      "what": "Dataset overview provides a high-level summary of the Oxford Pets dataset: number of images, breeds, species distribution, and data splits.",
      "why": "Understanding dataset structure is the first step in image classification EDA. It helps identify class balance, data quality, and potential challenges.",
      "how": "Check total images, breed distribution, train/val/test splits, and species balance. Look for class imbalance that might affect model performance."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Load data from GitHub (Full Dataset - 7,349 images)\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/full_metadata.csv'\ndf = pd.read_csv(url)\n\n# Basic info\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Total images: {len(df):,}\")\nprint(f\"Total breeds: {df['breed'].nunique()}\")\nprint(f\"Species: {df['species'].unique()}\")\n\n# Species distribution\nspecies_counts = df['species'].value_counts()\nprint(f\"\\nSpecies distribution:\")\nprint(species_counts)\n\n# Create pie chart for species\nfig = go.Figure(data=[go.Pie(\n    labels=species_counts.index,\n    values=species_counts.values,\n    hole=0.3,\n    marker=dict(colors=['#667eea', '#f093fb']),\n    textinfo='label+percent+value',\n    textfont_size=14\n)])\n\nfig.update_layout(\n    title='Species Distribution (Cats vs Dogs)',\n    showlegend=True,\n    width=600,\n    height=400\n)\n\nfig.show()\n\n# Split distribution\nsplit_counts = df['split'].value_counts()\nprint(f\"\\nSplit distribution:\")\nprint(split_counts)",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Basic info\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Total images: {len(df):,}\")\nprint(df.info())\n\n# Species distribution pie chart\nspecies_counts = df['species'].value_counts()\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Species pie chart\ncolors = ['#667eea', '#f093fb']\nwedges, texts, autotexts = ax1.pie(\n    species_counts.values,\n    labels=species_counts.index,\n    autopct='%1.1f%%',\n    colors=colors,\n    startangle=90\n)\n\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\n\nax1.set_title('Species Distribution', fontsize=14, fontweight='bold')\n\n# Split distribution bar chart\nsplit_counts = df['split'].value_counts()\nax2.bar(split_counts.index, split_counts.values, color='#4facfe', alpha=0.7)\nax2.set_title('Data Split Distribution', fontsize=14, fontweight='bold')\nax2.set_ylabel('Count')\n\n# Add value labels\nfor i, v in enumerate(split_counts.values):\n    ax2.text(i, v + 2, f'{v}', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Summary statistics\nprint(df.describe())\nprint(f\"\\nDataset shape: {df.shape}\")\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('Oxford Pets Dataset Overview', fontsize=16, fontweight='bold')\n\n# Species distribution\nspecies_counts = df['species'].value_counts()\nsns.barplot(x=species_counts.index, y=species_counts.values, \n           palette=['#667eea', '#f093fb'], ax=axes[0,0])\naxes[0,0].set_title('Species Distribution')\naxes[0,0].set_ylabel('Count')\n\n# Split distribution\nsplit_counts = df['split'].value_counts()\nsns.barplot(x=split_counts.index, y=split_counts.values, \n           palette='viridis', ax=axes[0,1])\naxes[0,1].set_title('Data Split Distribution')\naxes[0,1].set_ylabel('Count')\n\n# Breed count (top 10)\nbreed_counts = df['breed'].value_counts().head(10)\nsns.barplot(y=breed_counts.index, x=breed_counts.values, \n           palette='plasma', ax=axes[1,0])\naxes[1,0].set_title('Top 10 Breeds')\naxes[1,0].set_xlabel('Count')\n\n# Class ID distribution\nsns.histplot(data=df, x='class_id', bins=20, kde=True, ax=axes[1,1])\naxes[1,1].set_title('Class ID Distribution')\naxes[1,1].set_xlabel('Class ID')\n\nplt.tight_layout()\nplt.show()"
  },
  "breed_distribution": {
    "title": "Breed Distribution Analysis",
    "explanation": {
      "what": "Breed distribution analysis shows the frequency of each pet breed in the dataset, revealing class balance and potential challenges.",
      "why": "Understanding breed distribution is crucial for classification tasks. Imbalanced classes can bias model performance and require special handling.",
      "how": "Check breed counts, identify rare breeds (<5 samples), and look for class imbalance. Consider data augmentation or class weighting for rare breeds."
    },
    "plotly": "import pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Breed distribution\nbreed_counts = df['breed'].value_counts()\nprint(f\"Total breeds: {len(breed_counts)}\")\nprint(f\"\\nBreed distribution:\")\nprint(breed_counts)\n\n# Create horizontal bar chart\nfig = go.Figure(data=[go.Bar(\n    y=breed_counts.index,\n    x=breed_counts.values,\n    orientation='h',\n    marker_color='#667eea',\n    text=breed_counts.values,\n    textposition='outside'\n)])\n\nfig.update_layout(\n    title='Breed Distribution (All Breeds)',\n    xaxis_title='Count',\n    yaxis_title='Breed',\n    width=900,\n    height=600,\n    yaxis={'categoryorder': 'total ascending'}\n)\n\nfig.show()\n\n# Top 15 breeds\ntop_breeds = breed_counts.head(15)\nfig2 = go.Figure(data=[go.Bar(\n    x=top_breeds.index,\n    y=top_breeds.values,\n    marker_color='#4facfe',\n    text=top_breeds.values,\n    textposition='outside'\n)])\n\nfig2.update_layout(\n    title='Top 15 Breeds Distribution',\n    xaxis_title='Breed',\n    yaxis_title='Count',\n    width=900,\n    height=500,\n    xaxis={'tickangle': 45}\n)\n\nfig2.show()\n\n# Class imbalance analysis\nmin_samples = breed_counts.min()\nmax_samples = breed_counts.max()\nimbalance_ratio = max_samples / min_samples\n\nprint(f\"\\nClass Imbalance Analysis:\")\nprint(f\"Min samples: {min_samples}\")\nprint(f\"Max samples: {max_samples}\")\nprint(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n\nif imbalance_ratio > 2:\n    print(\"⚠️ Significant class imbalance detected!\")\n    print(\"Consider: data augmentation, class weighting, or resampling\")",
    "matplotlib": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Breed distribution\nbreed_counts = df['breed'].value_counts()\n\n# Create subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\nfig.suptitle('Breed Distribution Analysis', fontsize=16, fontweight='bold')\n\n# All breeds (horizontal bar)\nbreed_counts.plot(kind='barh', ax=ax1, color='#667eea', alpha=0.7)\nax1.set_title('All Breeds Distribution', fontsize=14, fontweight='bold')\nax1.set_xlabel('Count')\nax1.set_ylabel('Breed')\n\n# Top 15 breeds\ntop_breeds = breed_counts.head(15)\ntop_breeds.plot(kind='bar', ax=ax2, color='#4facfe', alpha=0.7)\nax2.set_title('Top 15 Breeds', fontsize=14, fontweight='bold')\nax2.set_xlabel('Breed')\nax2.set_ylabel('Count')\nax2.tick_params(axis='x', rotation=45)\n\n# Add value labels\nfor i, v in enumerate(top_breeds.values):\n    ax2.text(i, v + 0.5, f'{v}', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Class imbalance analysis\nmin_samples = breed_counts.min()\nmax_samples = breed_counts.max()\nimbalance_ratio = max_samples / min_samples\n\nprint(f\"\\nClass Imbalance Analysis:\")\nprint(f\"Min samples: {min_samples}\")\nprint(f\"Max samples: {max_samples}\")\nprint(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n\n# Rare breeds (< 5 samples)\nrare_breeds = breed_counts[breed_counts < 5]\nif len(rare_breeds) > 0:\n    print(f\"\\nRare breeds (< 5 samples): {len(rare_breeds)}\")\n    print(rare_breeds)",
    "seaborn": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Breed distribution\nbreed_counts = df['breed'].value_counts()\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Breed Distribution Analysis', fontsize=16, fontweight='bold')\n\n# All breeds horizontal bar\nbreed_counts.plot(kind='barh', ax=axes[0,0], color='#667eea', alpha=0.7)\naxes[0,0].set_title('All Breeds Distribution')\naxes[0,0].set_xlabel('Count')\n\n# Top 15 breeds\ntop_breeds = breed_counts.head(15)\nsns.barplot(x=top_breeds.values, y=top_breeds.index, \n           palette='viridis', ax=axes[0,1])\naxes[0,1].set_title('Top 15 Breeds')\naxes[0,1].set_xlabel('Count')\n\n# Species vs Breed heatmap\nspecies_breed = df.groupby(['species', 'breed']).size().unstack(fill_value=0)\nsns.heatmap(species_breed.T, annot=True, fmt='d', cmap='Blues', ax=axes[1,0])\naxes[1,0].set_title('Species vs Breed Heatmap')\naxes[1,0].set_xlabel('Species')\n\n# Class imbalance visualization\nimbalance_data = pd.DataFrame({\n    'Breed': breed_counts.index,\n    'Count': breed_counts.values\n})\nimbalance_data['Log_Count'] = np.log10(imbalance_data['Count'])\n\nsns.scatterplot(data=imbalance_data, x='Breed', y='Log_Count', \n               hue='Count', size='Count', sizes=(50, 200), ax=axes[1,1])\naxes[1,1].set_title('Class Imbalance (Log Scale)')\naxes[1,1].set_xlabel('Breed')\naxes[1,1].set_ylabel('Log Count')\naxes[1,1].tick_params(axis='x', rotation=90)\n\nplt.tight_layout()\nplt.show()\n\n# Statistics\nprint(f\"\\nBreed Distribution Statistics:\")\nprint(f\"Total breeds: {len(breed_counts)}\")\nprint(f\"Min samples: {breed_counts.min()}\")\nprint(f\"Max samples: {breed_counts.max()}\")\nprint(f\"Mean samples: {breed_counts.mean():.2f}\")\nprint(f\"Std samples: {breed_counts.std():.2f}\")"
  },
  "feature_extraction": {
    "title": "Feature Extraction Analysis",
    "explanation": {
      "what": "Feature extraction analysis shows how deep learning models (ResNet50) extract meaningful features from pet images for classification.",
      "why": "Understanding feature extraction helps explain model decisions and identify which image characteristics are most important for breed classification.",
      "how": "Use pre-trained ResNet50 to extract features, visualize feature distributions, and analyze feature similarity between breeds."
    },
    "plotly": "import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Load pre-trained ResNet50\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\n\n# Define image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\n\ndef extract_features(image_url):\n    \"\"\"Extract features from image using ResNet50\"\"\"\n    try:\n        response = requests.get(image_url)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n        image_tensor = transform(image).unsqueeze(0)\n        \n        with torch.no_grad():\n            features = model(image_tensor)\n            return features.numpy().flatten()\n    except:\n        return np.zeros(1000)  # Fallback for failed downloads\n\n# Extract features for sample images (first 50 for demo)\nprint(\"Extracting features from sample images...\")\nsample_df = df.head(50)\nfeatures_list = []\n\nfor idx, row in sample_df.iterrows():\n    # Construct image URL (this would be actual image path in real scenario)\n    image_url = f\"https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_images/{row['species']}/{row['image_name']}.jpg\"\n    features = extract_features(image_url)\n    features_list.append(features)\n\n# Convert to numpy array\nfeatures_array = np.array(features_list)\nprint(f\"Features shape: {features_array.shape}\")\n\n# PCA for dimensionality reduction\npca = PCA(n_components=2)\nfeatures_2d = pca.fit_transform(features_array)\n\n# Create scatter plot\nfig = go.Figure()\n\n# Color by breed\nfor breed in sample_df['breed'].unique():\n    breed_mask = sample_df['breed'] == breed\n    breed_features = features_2d[breed_mask]\n    \n    fig.add_trace(go.Scatter(\n        x=breed_features[:, 0],\n        y=breed_features[:, 1],\n        mode='markers',\n        name=breed,\n        text=sample_df[breed_mask]['image_name'],\n        hovertemplate='<b>%{text}</b><br>Breed: ' + breed + '<extra></extra>'\n    ))\n\nfig.update_layout(\n    title='Feature Extraction: ResNet50 Features (PCA)',\n    xaxis_title=f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)',\n    yaxis_title=f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)',\n    width=800,\n    height=600\n)\n\nfig.show()\n\n# Feature statistics\nprint(f\"\\nFeature Extraction Statistics:\")\nprint(f\"Original features: {features_array.shape[1]}\")\nprint(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\nprint(f\"Features per breed: {len(sample_df) / len(sample_df['breed'].unique()):.1f}\")",
    "matplotlib": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Simulate feature extraction (in real scenario, use ResNet50)\nnp.random.seed(42)\nn_samples = 100\nn_features = 2048  # ResNet50 output features\n\n# Generate synthetic features for demo\nfeatures = np.random.randn(n_samples, n_features)\nbreeds = df['breed'].head(n_samples).values\n\n# PCA for dimensionality reduction\npca = PCA(n_components=2)\nfeatures_2d = pca.fit_transform(features)\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Feature Extraction Analysis', fontsize=16, fontweight='bold')\n\n# PCA scatter plot\nunique_breeds = np.unique(breeds)\ncolors = plt.cm.tab20(np.linspace(0, 1, len(unique_breeds)))\n\nfor i, breed in enumerate(unique_breeds):\n    mask = breeds == breed\n    axes[0,0].scatter(features_2d[mask, 0], features_2d[mask, 1], \n                     c=[colors[i]], label=breed, alpha=0.7)\n\naxes[0,0].set_title('PCA: ResNet50 Features')\naxes[0,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\naxes[0,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\naxes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# t-SNE visualization\ntsne = TSNE(n_components=2, random_state=42)\nfeatures_tsne = tsne.fit_transform(features)\n\nfor i, breed in enumerate(unique_breeds):\n    mask = breeds == breed\n    axes[0,1].scatter(features_tsne[mask, 0], features_tsne[mask, 1], \n                     c=[colors[i]], label=breed, alpha=0.7)\n\naxes[0,1].set_title('t-SNE: ResNet50 Features')\naxes[0,1].set_xlabel('t-SNE 1')\naxes[0,1].set_ylabel('t-SNE 2')\n\n# Feature variance\nfeature_variance = np.var(features, axis=0)\naxes[1,0].hist(feature_variance, bins=50, alpha=0.7, color='#667eea')\naxes[1,0].set_title('Feature Variance Distribution')\naxes[1,0].set_xlabel('Variance')\naxes[1,0].set_ylabel('Frequency')\n\n# Explained variance\npca_full = PCA()\npca_full.fit(features)\nexplained_var = pca_full.explained_variance_ratio_\n\naxes[1,1].plot(np.cumsum(explained_var[:50]), 'o-', color='#4facfe')\naxes[1,1].set_title('Cumulative Explained Variance')\naxes[1,1].set_xlabel('Number of Components')\naxes[1,1].set_ylabel('Cumulative Variance')\naxes[1,1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFeature Extraction Statistics:\")\nprint(f\"Original features: {n_features}\")\nprint(f\"PCA explained variance (2D): {pca.explained_variance_ratio_.sum():.2%}\")\nprint(f\"Top 10 components explain: {np.cumsum(explained_var[:10]).sum():.2%}\")",
    "seaborn": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Simulate feature extraction\nnp.random.seed(42)\nn_samples = 100\nn_features = 2048\n\n# Generate synthetic features\nfeatures = np.random.randn(n_samples, n_features)\nbreeds = df['breed'].head(n_samples).values\n\n# Create comprehensive analysis\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Feature Extraction Analysis', fontsize=16, fontweight='bold')\n\n# PCA scatter plot\npca = PCA(n_components=2)\nfeatures_2d = pca.fit_transform(features)\n\nunique_breeds = np.unique(breeds)\nfor breed in unique_breeds:\n    mask = breeds == breed\n    axes[0,0].scatter(features_2d[mask, 0], features_2d[mask, 1], \n                     label=breed, alpha=0.7)\n\naxes[0,0].set_title('PCA: Feature Distribution')\naxes[0,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\naxes[0,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n\n# t-SNE visualization\ntsne = TSNE(n_components=2, random_state=42)\nfeatures_tsne = tsne.fit_transform(features)\n\nfor breed in unique_breeds:\n    mask = breeds == breed\n    axes[0,1].scatter(features_tsne[mask, 0], features_tsne[mask, 1], \n                     label=breed, alpha=0.7)\n\naxes[0,1].set_title('t-SNE: Feature Clustering')\naxes[0,1].set_xlabel('t-SNE 1')\naxes[0,1].set_ylabel('t-SNE 2')\n\n# Feature correlation heatmap (sample)\nfeature_sample = features[:20, :50]  # Sample for visualization\ncorr_matrix = np.corrcoef(feature_sample.T)\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0, ax=axes[0,2])\naxes[0,2].set_title('Feature Correlation (Sample)')\n\n# Feature variance by breed\nbreed_variance = []\nbreed_labels = []\nfor breed in unique_breeds:\n    mask = breeds == breed\n    breed_features = features[mask]\n    variance = np.var(breed_features, axis=0)\n    breed_variance.extend(variance)\n    breed_labels.extend([breed] * len(variance))\n\nvariance_df = pd.DataFrame({'Variance': breed_variance, 'Breed': breed_labels})\nsns.boxplot(data=variance_df, x='Breed', y='Variance', ax=axes[1,0])\naxes[1,0].set_title('Feature Variance by Breed')\naxes[1,0].tick_params(axis='x', rotation=45)\n\n# Explained variance\npca_full = PCA()\npca_full.fit(features)\nexplained_var = pca_full.explained_variance_ratio_\n\naxes[1,1].plot(np.cumsum(explained_var[:50]), 'o-', color='#667eea')\naxes[1,1].set_title('Cumulative Explained Variance')\naxes[1,1].set_xlabel('Components')\naxes[1,1].set_ylabel('Cumulative Variance')\naxes[1,1].grid(True, alpha=0.3)\n\n# Feature similarity matrix\nsimilarity_matrix = cosine_similarity(features[:20])  # Sample for performance\nsns.heatmap(similarity_matrix, cmap='viridis', ax=axes[1,2])\naxes[1,2].set_title('Feature Similarity Matrix')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFeature Extraction Statistics:\")\nprint(f\"Original features: {n_features}\")\nprint(f\"PCA explained variance (2D): {pca.explained_variance_ratio_.sum():.2%}\")\nprint(f\"Top 10 components: {np.cumsum(explained_var[:10]).sum():.2%}\")"
  },
  "similarity_analysis": {
    "title": "Breed Similarity Analysis",
    "explanation": {
      "what": "Breed similarity analysis measures how similar different pet breeds are based on their visual features, revealing breed relationships and potential confusion.",
      "why": "Understanding breed similarities helps identify which breeds are most likely to be confused by the model and guides data augmentation strategies.",
      "how": "Compute cosine similarity between breed features, create similarity heatmaps, and identify breed clusters that are visually similar."
    },
    "plotly": "import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import AgglomerativeClustering\nimport networkx as nx\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Simulate breed features (in real scenario, use actual ResNet50 features)\nnp.random.seed(42)\nbreeds = df['breed'].unique()\nn_breeds = len(breeds)\n\n# Generate synthetic breed features\nbreed_features = np.random.randn(n_breeds, 2048)\n\n# Compute similarity matrix\nsimilarity_matrix = cosine_similarity(breed_features)\n\n# Create similarity heatmap\nfig = go.Figure(data=go.Heatmap(\n    z=similarity_matrix,\n    x=breeds,\n    y=breeds,\n    colorscale='RdBu',\n    zmid=0,\n    text=np.round(similarity_matrix, 2),\n    texttemplate='%{text}',\n    textfont={'size': 8}\n))\n\nfig.update_layout(\n    title='Breed Similarity Matrix (Cosine Similarity)',\n    xaxis_title='Breeds',\n    yaxis_title='Breeds',\n    width=800,\n    height=700\n)\n\nfig.show()\n\n# Find most similar breed pairs\nsimilarity_pairs = []\nfor i in range(n_breeds):\n    for j in range(i+1, n_breeds):\n        similarity_pairs.append({\n            'breed1': breeds[i],\n            'breed2': breeds[j],\n            'similarity': similarity_matrix[i, j]\n        })\n\nsimilarity_df = pd.DataFrame(similarity_pairs)\ntop_similar = similarity_df.nlargest(10, 'similarity')\n\nprint(\"\\nTop 10 Most Similar Breed Pairs:\")\nprint(top_similar)\n\n# Create network graph of similar breeds\nG = nx.Graph()\nthreshold = 0.7  # Similarity threshold\n\nfor i in range(n_breeds):\n    for j in range(i+1, n_breeds):\n        if similarity_matrix[i, j] > threshold:\n            G.add_edge(breeds[i], breeds[j], weight=similarity_matrix[i, j])\n\n# Plot network\npos = nx.spring_layout(G, k=1, iterations=50)\nedge_x = []\nedge_y = []\nedge_info = []\n\nfor edge in G.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n    edge_info.append(f\"{edge[0]} - {edge[1]}: {G[edge[0]][edge[1]]['weight']:.3f}\")\n\n# Create network plot\nfig2 = go.Figure()\n\n# Add edges\nfig2.add_trace(go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=2, color='#888'),\n    hoverinfo='none',\n    mode='lines',\n    name='Similarity'\n))\n\n# Add nodes\nnode_x = [pos[node][0] for node in G.nodes()]\nnode_y = [pos[node][1] for node in G.nodes()]\nnode_text = list(G.nodes())\n\nfig2.add_trace(go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers+text',\n    marker=dict(size=20, color='#667eea'),\n    text=node_text,\n    textposition='middle center',\n    name='Breeds'\n))\n\nfig2.update_layout(\n    title=f'Breed Similarity Network (threshold > {threshold})',\n    showlegend=False,\n    hovermode='closest',\n    margin=dict(b=20,l=5,r=5,t=40),\n    annotations=[ dict(\n        text=\"Similar breeds are connected by edges\",\n        showarrow=False,\n        xref=\"paper\", yref=\"paper\",\n        x=0.005, y=-0.002,\n        xanchor='left', yanchor='bottom',\n        font=dict(color='#666', size=12)\n    )],\n    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n    width=800,\n    height=600\n)\n\nfig2.show()\n\n# Clustering analysis\nclustering = AgglomerativeClustering(n_clusters=5, affinity='precomputed', linkage='average')\ndistance_matrix = 1 - similarity_matrix\ncluster_labels = clustering.fit_predict(distance_matrix)\n\ncluster_df = pd.DataFrame({\n    'breed': breeds,\n    'cluster': cluster_labels\n})\n\nprint(f\"\\nBreed Clusters:\")\nfor cluster_id in range(5):\n    cluster_breeds = cluster_df[cluster_df['cluster'] == cluster_id]['breed'].tolist()\n    print(f\"Cluster {cluster_id}: {cluster_breeds}\")",
    "matplotlib": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import AgglomerativeClustering\nimport seaborn as sns\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Simulate breed features\nnp.random.seed(42)\nbreeds = df['breed'].unique()\nn_breeds = len(breeds)\nbreed_features = np.random.randn(n_breeds, 2048)\n\n# Compute similarity matrix\nsimilarity_matrix = cosine_similarity(breed_features)\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Breed Similarity Analysis', fontsize=16, fontweight='bold')\n\n# Similarity heatmap\nim = axes[0,0].imshow(similarity_matrix, cmap='RdBu', vmin=-1, vmax=1)\naxes[0,0].set_title('Breed Similarity Matrix')\naxes[0,0].set_xlabel('Breeds')\naxes[0,0].set_ylabel('Breeds')\naxes[0,0].set_xticks(range(len(breeds)))\naxes[0,0].set_yticks(range(len(breeds)))\naxes[0,0].set_xticklabels(breeds, rotation=45, ha='right')\naxes[0,0].set_yticklabels(breeds)\n\n# Add colorbar\nplt.colorbar(im, ax=axes[0,0], shrink=0.8)\n\n# Top similar pairs\nsimilarity_pairs = []\nfor i in range(n_breeds):\n    for j in range(i+1, n_breeds):\n        similarity_pairs.append({\n            'breed1': breeds[i],\n            'breed2': breeds[j],\n            'similarity': similarity_matrix[i, j]\n        })\n\nsimilarity_df = pd.DataFrame(similarity_pairs)\ntop_similar = similarity_df.nlargest(10, 'similarity')\n\ny_pos = np.arange(len(top_similar))\naxes[0,1].barh(y_pos, top_similar['similarity'], color='#667eea', alpha=0.7)\naxes[0,1].set_yticks(y_pos)\naxes[0,1].set_yticklabels([f\"{row['breed1']} - {row['breed2']}\" for _, row in top_similar.iterrows()])\naxes[0,1].set_title('Top 10 Similar Breed Pairs')\naxes[0,1].set_xlabel('Cosine Similarity')\n\n# Similarity distribution\nsimilarity_values = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\naxes[1,0].hist(similarity_values, bins=30, alpha=0.7, color='#4facfe', edgecolor='black')\naxes[1,0].set_title('Similarity Distribution')\naxes[1,0].set_xlabel('Cosine Similarity')\naxes[1,0].set_ylabel('Frequency')\naxes[1,0].axvline(np.mean(similarity_values), color='red', linestyle='--', \n                 label=f'Mean: {np.mean(similarity_values):.3f}')\naxes[1,0].legend()\n\n# Clustering dendrogram\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\n\ndistance_matrix = 1 - similarity_matrix\nlinkage_matrix = linkage(squareform(distance_matrix), method='average')\ndendrogram(linkage_matrix, labels=breeds, ax=axes[1,1], orientation='top')\naxes[1,1].set_title('Breed Clustering Dendrogram')\naxes[1,1].set_xlabel('Breeds')\naxes[1,1].set_ylabel('Distance')\n\nplt.tight_layout()\nplt.show()\n\n# Statistics\nprint(f\"\\nSimilarity Analysis Statistics:\")\nprint(f\"Mean similarity: {np.mean(similarity_values):.3f}\")\nprint(f\"Std similarity: {np.std(similarity_values):.3f}\")\nprint(f\"Max similarity: {np.max(similarity_values):.3f}\")\nprint(f\"Min similarity: {np.min(similarity_values):.3f}\")\n\n# Most similar pairs\nprint(f\"\\nTop 5 Most Similar Pairs:\")\nfor _, row in top_similar.head(5).iterrows():\n    print(f\"{row['breed1']} - {row['breed2']}: {row['similarity']:.3f}\")",
    "seaborn": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\n\n# Load data\nurl = 'https://raw.githubusercontent.com/LTSACH/AILearningHub/main/datasets/oxford-pets/data/sample_annotations/csv/classification.csv'\ndf = pd.read_csv(url)\n\n# Simulate breed features\nnp.random.seed(42)\nbreeds = df['breed'].unique()\nn_breeds = len(breeds)\nbreed_features = np.random.randn(n_breeds, 2048)\n\n# Compute similarity matrix\nsimilarity_matrix = cosine_similarity(breed_features)\n\n# Create comprehensive analysis\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\nfig.suptitle('Breed Similarity Analysis', fontsize=16, fontweight='bold')\n\n# Similarity heatmap\nsns.heatmap(similarity_matrix, \n           xticklabels=breeds, \n           yticklabels=breeds,\n           cmap='RdBu', \n           center=0,\n           square=True,\n           ax=axes[0,0])\naxes[0,0].set_title('Similarity Heatmap')\n\n# Similarity distribution\nsimilarity_values = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\nsns.histplot(similarity_values, bins=30, kde=True, ax=axes[0,1])\naxes[0,1].set_title('Similarity Distribution')\naxes[0,1].set_xlabel('Cosine Similarity')\naxes[0,1].axvline(np.mean(similarity_values), color='red', linestyle='--', \n                 label=f'Mean: {np.mean(similarity_values):.3f}')\naxes[0,1].legend()\n\n# Top similar pairs\nsimilarity_pairs = []\nfor i in range(n_breeds):\n    for j in range(i+1, n_breeds):\n        similarity_pairs.append({\n            'breed1': breeds[i],\n            'breed2': breeds[j],\n            'similarity': similarity_matrix[i, j]\n        })\n\nsimilarity_df = pd.DataFrame(similarity_pairs)\ntop_similar = similarity_df.nlargest(15, 'similarity')\n\nsns.barplot(data=top_similar, x='similarity', y='breed1', \n           hue='breed2', ax=axes[0,2])\naxes[0,2].set_title('Top Similar Pairs')\naxes[0,2].set_xlabel('Cosine Similarity')\n\n# Clustering dendrogram\ndistance_matrix = 1 - similarity_matrix\nlinkage_matrix = linkage(squareform(distance_matrix), method='average')\ndendrogram(linkage_matrix, labels=breeds, ax=axes[1,0], orientation='top')\naxes[1,0].set_title('Breed Clustering')\n\n# Similarity by species\nspecies_similarity = []\nfor i in range(n_breeds):\n    for j in range(i+1, n_breeds):\n        breed1_species = df[df['breed'] == breeds[i]]['species'].iloc[0]\n        breed2_species = df[df['breed'] == breeds[j]]['species'].iloc[0]\n        species_similarity.append({\n            'similarity': similarity_matrix[i, j],\n            'species_pair': f\"{breed1_species}-{breed2_species}\"\n        })\n\nspecies_sim_df = pd.DataFrame(species_similarity)\nsns.boxplot(data=species_sim_df, x='species_pair', y='similarity', ax=axes[1,1])\naxes[1,1].set_title('Similarity by Species Pairs')\naxes[1,1].set_xlabel('Species Pairs')\naxes[1,1].tick_params(axis='x', rotation=45)\n\n# Similarity network (simplified)\nthreshold = 0.7\nhigh_similarity = similarity_matrix > threshold\nnp.fill_diagonal(high_similarity, False)\n\n# Create adjacency matrix for network\nadjacency_matrix = high_similarity.astype(int)\nsns.heatmap(adjacency_matrix, \n           xticklabels=breeds, \n           yticklabels=breeds,\n           cmap='Blues',\n           square=True,\n           ax=axes[1,2])\naxes[1,2].set_title(f'High Similarity Network (>{threshold})')\n\nplt.tight_layout()\nplt.show()\n\n# Detailed statistics\nprint(f\"\\nSimilarity Analysis Statistics:\")\nprint(f\"Mean similarity: {np.mean(similarity_values):.3f}\")\nprint(f\"Std similarity: {np.std(similarity_values):.3f}\")\nprint(f\"Max similarity: {np.max(similarity_values):.3f}\")\nprint(f\"Min similarity: {np.min(similarity_values):.3f}\")\n\n# Most similar pairs\nprint(f\"\\nTop 10 Most Similar Pairs:\")\nfor _, row in top_similar.head(10).iterrows():\n    print(f\"{row['breed1']} - {row['breed2']}: {row['similarity']:.3f}\")\n\n# Clustering results\nclustering = AgglomerativeClustering(n_clusters=5, affinity='precomputed', linkage='average')\ncluster_labels = clustering.fit_predict(distance_matrix)\n\ncluster_df = pd.DataFrame({\n    'breed': breeds,\n    'cluster': cluster_labels\n})\n\nprint(f\"\\nBreed Clusters:\")\nfor cluster_id in range(5):\n    cluster_breeds = cluster_df[cluster_df['cluster'] == cluster_id]['breed'].tolist()\n    print(f\"Cluster {cluster_id}: {cluster_breeds}\")"
  }
}